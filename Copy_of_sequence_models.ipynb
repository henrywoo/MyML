{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of sequence_models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JndnmDMp66FL",
        "UucW-jmeoC31",
        "HxVt7a30h8t6"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrywoo/MyML/blob/master/Copy_of_sequence_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "JndnmDMp66FL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Copyright 2018 Google LLC."
      ]
    },
    {
      "metadata": {
        "id": "hMqWDc_m6rUC",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PTaAdgy3LS8W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sequence Models\n",
        "\n",
        "**Learning Objectives:**\n",
        "* Parse and prepare movie review string data for sentiment analysis\n",
        "* Create a custom Estimator for a simple dynamic Recurrent Neural Network (RNN)\n",
        "* Train and test an RNN model\n",
        "\n",
        "In this Colab, we'll work with embeddings using text data from movie reviews (from the [ACL 2011 IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/)). This data has already been processed into the `tf.Example` format.\n",
        "\n",
        "Please **make a copy** of this Colab notebook before starting this lab. To do so, choose **File**->**Save a copy in Drive**."
      ]
    },
    {
      "metadata": {
        "id": "2AKGtmwNosU8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Let's import our dependencies and download the training and test data. [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) includes a file download and caching tool that we can use to retrieve the data sets."
      ]
    },
    {
      "metadata": {
        "id": "jGWqDqFFL_NZ",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "import collections\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from IPython import display\n",
        "from sklearn import metrics\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "train_url = 'https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/train.tfrecord'\n",
        "train_path = tf.keras.utils.get_file(train_url.split('/')[-1], train_url)\n",
        "test_url = 'https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/test.tfrecord'\n",
        "test_path = tf.keras.utils.get_file(test_url.split('/')[-1], test_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6W7aZ9qspZVj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis and Movie Review Classification"
      ]
    },
    {
      "metadata": {
        "id": "jieA0k_NLS8a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this lab, we'll build a classifier to assign sentiment polarity labels to short movie reviews from IMDB: 1 if it's overall positive or 0 if overall negative.\n",
        "\n",
        "The dataset consists of 50K movie reviews, split equally into test and train sets. The data is also split equally between positives and negatives in both train and test sets. It has already been pre-processed, and each entry contains an ordered list of \"terms\" from the review, along with a label."
      ]
    },
    {
      "metadata": {
        "id": "Uvoa2HyDtgqe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building the Input Pipeline"
      ]
    },
    {
      "metadata": {
        "id": "O20vMEOurDol",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, let's configure the input pipeline to import our data into a TensorFlow model. We can use the following function to parse the training and test data (which is in [TFRecord](https://www.tensorflow.org/programmers_guide/datasets) format) and return a dict of the features and the corresponding labels."
      ]
    },
    {
      "metadata": {
        "id": "SxxNIEniPq2z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _parse_function(record):\n",
        "  \"\"\"Extracts features and labels.\n",
        "  \n",
        "  Args:\n",
        "    record: File path to a TFRecord file    \n",
        "  Returns:\n",
        "    A `tuple` `(labels, features)`:\n",
        "      features: A dict of tensors representing the features\n",
        "      labels: A tensor with the corresponding labels.\n",
        "  \"\"\"\n",
        "  features = {\n",
        "    \"terms\": tf.VarLenFeature(dtype=tf.string), # terms are strings of varying lengths\n",
        "    \"labels\": tf.FixedLenFeature(shape=[1], dtype=tf.float32) # labels are 0 or 1\n",
        "  }\n",
        "  \n",
        "  parsed_features = tf.parse_single_example(record, features)\n",
        "  \n",
        "  terms = parsed_features['terms'].values\n",
        "  labels = parsed_features['labels']\n",
        "\n",
        "  return  {'terms':terms}, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SXhTeeYMrp-l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To confirm our function is working as expected, let's construct a `TFRecordDataset` for the training data, and map the data to features and labels using the function above."
      ]
    },
    {
      "metadata": {
        "id": "FFFGWvIn4o7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Graph().as_default():\n",
        "  # Create the Dataset object.\n",
        "  ds = tf.data.TFRecordDataset(train_path)\n",
        "  # Map features and labels with the parse function.\n",
        "  ds = ds.map(_parse_function)\n",
        "  nxt = ds.make_one_shot_iterator().get_next()\n",
        "  sess = tf.Session()\n",
        "\n",
        "print 'Dataset : {}\\n'.format(ds)\n",
        "print 'Example : {}'.format(sess.run(nxt))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jBU39UeFty9S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's build a formal input function that we can pass to the `train()` method of a TensorFlow Estimator object."
      ]
    },
    {
      "metadata": {
        "id": "UBHHHse45YNe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create an input_fn that parses the tf.Examples from the given files,\n",
        "# and split them into features and targets.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def my_input_fn(input_filenames, num_epochs=None, shuffle=True):\n",
        "  \n",
        "  # Same code as above; create a dataset and map features and labels.\n",
        "  ds = tf.data.TFRecordDataset(input_filenames)\n",
        "  ds = ds.map(_parse_function)\n",
        "  ds = ds.repeat(num_epochs)\n",
        "\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(10 * batch_size)\n",
        "\n",
        "  # Our feature data is variable-length, so we pad and batch\n",
        "  # each field of the dataset structure to whatever size is necessary.\n",
        "  ds = ds.padded_batch(batch_size, ds.output_shapes)\n",
        "  ds = ds.prefetch(8)\n",
        "  \n",
        "  # Return the next batch of data.\n",
        "  features, labels = ds.make_one_shot_iterator().get_next()\n",
        "  return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y170tVlrLS8c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1: Create a custom Estimator for a Dynamic Recurrent Neural Network (RNN)\n",
        "\n",
        "Next, we'll build a custom Estimator for a Dynamic RNN. We'll start with a simple 1-layer LSTM with 32 hidden units.\n",
        "\n",
        "Please consult the documentation available on [Creating Custom Estimators](https://www.tensorflow.org/get_started/custom_estimators) in TensorFlow for more information on how to set up your model_fn with the model definition (including [input](https://www.tensorflow.org/get_started/custom_estimators#define_the_input_layer), [hidden](https://www.tensorflow.org/get_started/custom_estimators#hidden_layers) and [output](https://www.tensorflow.org/get_started/custom_estimators#output_layer) layers), as well as the branching code implementing [prediction](https://www.tensorflow.org/get_started/custom_estimators#predict), [evaluation](https://www.tensorflow.org/get_started/custom_estimators#evaluate) and [training](https://www.tensorflow.org/get_started/custom_estimators#train).\n",
        "\n",
        "For additional helpful references on RNNs, please check the tutorial on [Recurrent Neural Networks](https://www.tensorflow.org/tutorials/recurrent). A variety of cell types is available; we recommend starting with the [Long Short-Term Memory unit](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell) recurrent network cell."
      ]
    },
    {
      "metadata": {
        "id": "XX1rHqAd57ZB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_hash_buckets = 1000\n",
        "embedding_dim = 32\n",
        "rnn_cell_num_units = 32\n",
        "\n",
        "def my_model_fn(features, labels, mode, params):\n",
        "  \"\"\"Custom estimator for Dynamic RNN.\n",
        "\n",
        "  Args:\n",
        "    features: This is batch_features from input_fn.\n",
        "    labels: This is batch_labels from input_fn.\n",
        "    mode: An instance of tf.estimator.ModeKeys.\n",
        "    params: Additional parameters.\n",
        "  Returns:\n",
        "    A a tf.estimator.EstimatorSpec\n",
        "  \"\"\"\n",
        "  tf.logging.info('my_model_fn: {}'.format(mode))\n",
        "    \n",
        "  tokens = features['terms']\n",
        "  token_ids = tf.string_to_hash_bucket_fast(tokens, embedding_hash_buckets)\n",
        "  \n",
        "  embeddings = tf.get_variable(\"word_embeddings\",\n",
        "                               [embedding_hash_buckets, embedding_dim])\n",
        "  \n",
        "  token_embeddings = tf.nn.embedding_lookup(embeddings, token_ids)\n",
        "\n",
        "  # Define the model here.\n",
        "  # WRITE YOUR CODE HERE\n",
        "\n",
        "  # Prediction mode.\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    return tf.estimator.EstimatorSpec(mode,\n",
        "                                      predictions={\n",
        "                                          'probabilities': probabilities,\n",
        "                                          'predictions': predictions})\n",
        "\n",
        "  # Evaluation and training modes.\n",
        "  # Calculate loss.\n",
        "  loss = tf.losses.sigmoid_cross_entropy(labels, logits=logits)\n",
        "  \n",
        "  # Calculate the accuracy between the true labels and our predictions.\n",
        "  accuracy = tf.metrics.accuracy(tf.to_int32(labels), predictions)\n",
        "\n",
        "  # Evaluation mode.\n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode,\n",
        "        loss=loss,\n",
        "        eval_metric_ops={'my_accuracy': accuracy})\n",
        "\n",
        "  # If mode is not PREDICT nor EVAL, then it must be TRAIN.\n",
        "  assert mode == tf.estimator.ModeKeys.TRAIN, 'TRAIN is only ModeKey left.'\n",
        "\n",
        "  # Training mode.\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
        "#   optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, 5.0)\n",
        "  train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
        "\n",
        "\n",
        "  # Return training operations: loss and train_op.\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode,\n",
        "      loss=loss,\n",
        "      train_op=train_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UucW-jmeoC31",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Possible solution"
      ]
    },
    {
      "metadata": {
        "id": "s48y6PX4oG3m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_hash_buckets = 1000\n",
        "embedding_dim = 32\n",
        "rnn_cell_num_units = 32\n",
        "\n",
        "def my_model_fn(features, labels, mode, params):\n",
        "  \"\"\"Custom estimator for Dynamic RNN.\n",
        "\n",
        "  Args:\n",
        "    features: This is batch_features from input_fn.\n",
        "    labels: This is batch_labels from input_fn.\n",
        "    mode: An instance of tf.estimator.ModeKeys.\n",
        "    params: Additional parameters.\n",
        "  Returns:\n",
        "    A a tf.estimator.EstimatorSpec\n",
        "  \"\"\"\n",
        "  tf.logging.info('my_model_fn: {}'.format(mode))\n",
        "    \n",
        "  tokens = features['terms']\n",
        "  token_ids = tf.string_to_hash_bucket_fast(tokens, embedding_hash_buckets)\n",
        "  \n",
        "  embeddings = tf.get_variable(\"word_embeddings\",\n",
        "                               [embedding_hash_buckets, embedding_dim])\n",
        "  \n",
        "  token_embeddings = tf.nn.embedding_lookup(embeddings, token_ids)\n",
        "\n",
        "  # Define the model here.\n",
        "  rnn_cell = tf.contrib.rnn.LSTMBlockCell(rnn_cell_num_units)\n",
        "  outputs, _ = tf.nn.dynamic_rnn(rnn_cell,\n",
        "                                 token_embeddings,\n",
        "                                 dtype=tf.float32)\n",
        "\n",
        "  # We only want the last output.\n",
        "  last_output = outputs[:, -1, :]\n",
        "  logits = tf.layers.dense(last_output, 1)\n",
        "  probabilities = tf.sigmoid(logits)\n",
        "  predictions = tf.to_int32(tf.round(probabilities))\n",
        "\n",
        "  # Prediction mode.\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    return tf.estimator.EstimatorSpec(mode,\n",
        "                                      predictions={\n",
        "                                          'probabilities': probabilities,\n",
        "                                          'predictions': predictions})\n",
        "\n",
        "  # Evaluation and training modes.\n",
        "  # Calculate loss.\n",
        "  loss = tf.losses.sigmoid_cross_entropy(labels, logits=logits)\n",
        "  \n",
        "  # Calculate the accuracy between the true labels and our predictions.\n",
        "  accuracy = tf.metrics.accuracy(tf.to_int32(labels), predictions)\n",
        "\n",
        "  # Evaluation mode.\n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode,\n",
        "        loss=loss,\n",
        "        eval_metric_ops={'my_accuracy': accuracy})\n",
        "\n",
        "  # If mode is not PREDICT nor EVAL, then it must be TRAIN.\n",
        "  assert mode == tf.estimator.ModeKeys.TRAIN, 'TRAIN is only ModeKey left.'\n",
        "\n",
        "  # Training mode.\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
        "#   optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, 5.0)\n",
        "  train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
        "\n",
        "\n",
        "  # Return training operations: loss and train_op.\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode,\n",
        "      loss=loss,\n",
        "      train_op=train_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J0ubn9gULS8g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 2: Use the Dynamic Recurrent Neural Network (RNN) for sentiment analysis\n",
        "\n",
        "Next, we'll construct the Dynamic RNN classifier, train it on the training set, and evaluate it on the evaluation set. After you read through the code, run it and see how you do."
      ]
    },
    {
      "metadata": {
        "id": "jcgOPfEALS8h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier = tf.estimator.Estimator(\n",
        "    model_fn=my_model_fn,\n",
        ")\n",
        "\n",
        "num_train_steps = 100\n",
        "num_eval_steps = 100\n",
        "\n",
        "classifier.train(\n",
        "    input_fn=lambda: my_input_fn([train_path]),\n",
        "    steps=num_train_steps)\n",
        "\n",
        "evaluation_metrics = classifier.evaluate(\n",
        "    input_fn=lambda: my_input_fn([train_path]),\n",
        "    steps=num_eval_steps)\n",
        "\n",
        "print 'Training set metrics:'\n",
        "for m in evaluation_metrics:\n",
        "  print m, evaluation_metrics[m]\n",
        "print '---'\n",
        "\n",
        "evaluation_metrics = classifier.evaluate(\n",
        "    input_fn=lambda: my_input_fn([test_path]),\n",
        "    steps=num_eval_steps)\n",
        "\n",
        "print 'Test set metrics:'\n",
        "for m in evaluation_metrics:\n",
        "  print m, evaluation_metrics[m]\n",
        "print '---'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pUb3L7pqLS86",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 3:  Try to improve the model's performance\n",
        "\n",
        "Now see if you can refine the model to improve performance! Here is a short list of a few things you can try to do:\n",
        "\n",
        "* **Adjusting hyperparameters** such as cell type, cell size, number of layers, embedding dimension, optimizer type, or learning rate.\n",
        "* **Using a vocabulary list** to keep only a limited set of most informative terms.\n",
        "* **Updating the call to tf.nn.dynamic_rnn in the custom estimator** to take sequence length as an input argument."
      ]
    },
    {
      "metadata": {
        "id": "iJKi4dDaGyHi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OyJehCsOhrhb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 4: Use the canned Estimator: RNNClassifier\n",
        "Now that we've written a custom Estimator, let's try the new canned Estimator, RNNClassifier and compare the two.\n",
        "\n",
        "RNNClassifier is defined [here](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/RNNClassifier).\n",
        "\n",
        "Start by defining some parameters:"
      ]
    },
    {
      "metadata": {
        "id": "jbFBMLPjhvBA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_hash_buckets = 1000\n",
        "embedding_dim = 32\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_V-ZrwUhxeH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When using the canned RNNClassifier, use the columns for sequence models defined in [sequence_feature_columns.py](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column.py) in contrib. Start by defining a function to get feature columns here."
      ]
    },
    {
      "metadata": {
        "id": "A8qIsYMFh1Yt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_feature_columns():\n",
        "  # WRITE YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7nNS8ltWh3c5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, create your input function, which will parse the tf.Examples from the given files and split them into features and targets."
      ]
    },
    {
      "metadata": {
        "id": "Vg7HBnFCh4B1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def my_input_fn(input_filenames, num_epochs=None, shuffle=True):\n",
        "  # WRITE YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HxVt7a30h8t6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ]
    },
    {
      "metadata": {
        "id": "duVnxdzih9Ik",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_feature_columns():\n",
        "  terms = tf.contrib.feature_column.sequence_categorical_column_with_hash_bucket(\n",
        "      \"terms\", embedding_hash_buckets)\n",
        "  terms_embedding = tf.feature_column.embedding_column(terms, embedding_dim)\n",
        "  return [terms_embedding]\n",
        "\n",
        "\n",
        "def my_input_fn(input_filenames, num_epochs=None, shuffle=True):\n",
        "  \n",
        "  # Same code as above; create a dataset and map features and labels.\n",
        "  ds = tf.data.TFRecordDataset(input_filenames)\n",
        "  \n",
        "  # Use FeatureColumns to generate the parse_example spec.\n",
        "  feature_spec = tf.estimator.classifier_parse_example_spec(\n",
        "      get_feature_columns(),\n",
        "      label_key=\"labels\",\n",
        "      label_dtype=tf.float32)\n",
        "  ds = ds.map(lambda x: tf.parse_single_example(x, feature_spec))\n",
        "\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(10000)\n",
        "\n",
        "  # The canned Estimator handles padding internally and will skip computation\n",
        "  # on padded terms, so we don't need to pad our data in the input_fn.\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.repeat(num_epochs)\n",
        "\n",
        "  # Return the next batch of data.\n",
        "  features = ds.make_one_shot_iterator().get_next()\n",
        "  labels = features.pop(\"labels\")\n",
        "  return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "brjWlwpZiB-p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training and evaluating the model\n",
        "\n",
        "Now we can construct the RNNClassifier, train it on the training set, evaluate it on the evaluation set, and print out some metrics."
      ]
    },
    {
      "metadata": {
        "id": "6SMZZh-XiIRa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier = tf.contrib.estimator.RNNClassifier(\n",
        "    sequence_feature_columns=get_feature_columns(),\n",
        "    num_units=[32],\n",
        "    cell_type='lstm',\n",
        "    n_classes=2\n",
        ")\n",
        "\n",
        "num_train_steps = 100\n",
        "num_eval_steps = 100\n",
        "\n",
        "classifier.train(\n",
        "    input_fn=lambda: my_input_fn([train_path]),\n",
        "    steps=num_train_steps)\n",
        "\n",
        "evaluation_metrics = classifier.evaluate(\n",
        "    input_fn=lambda: my_input_fn([train_path]),\n",
        "    steps=num_eval_steps)\n",
        "\n",
        "print 'Training set metrics:'\n",
        "for m in evaluation_metrics:\n",
        "  print m, evaluation_metrics[m]\n",
        "print '---'\n",
        "\n",
        "evaluation_metrics = classifier.evaluate(\n",
        "    input_fn=lambda: my_input_fn([test_path]),\n",
        "    steps=num_eval_steps)\n",
        "\n",
        "print 'Test set metrics:'\n",
        "for m in evaluation_metrics:\n",
        "  print m, evaluation_metrics[m]\n",
        "print '---'"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}