{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of sequence_models_char.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "GJMr1556i3nF"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrywoo/MyML/blob/master/Copy_of_sequence_models_char.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GJMr1556i3nF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Copyright 2018 Google LLC."
      ]
    },
    {
      "metadata": {
        "id": "uLcZy_02jFcJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iw_w9-n3jJ7t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sequence Models: Text Generation Using a RNN\n",
        "\n",
        "**Learning Objectives:**\n",
        "* Generate text using a character-based RNN\n",
        "* Train and test a RNN model\n",
        "\n",
        "In this Colab, we'll work with a dataset of Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data (\"Shakespear\"), we'll train a model to predict the next character in the sequence (\"e\")."
      ]
    },
    {
      "metadata": {
        "id": "5q8dfoQoj1cU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before getting started, please do the following:\n",
        "\n",
        "1.   Make a copy of this Colab notebook. To do so, choose **File**->**Save a copy in Drive**.\n",
        "2.   Click on \"CONNECT\" in the top right corner, then choose **Runtime**->**Change runtime type**->**Hardware Accelerator: GPU**."
      ]
    },
    {
      "metadata": {
        "id": "XXFZny-9VRrm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n"
      ]
    },
    {
      "metadata": {
        "id": "FvhNtmG2kfQC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's run the next cell to import the libraries."
      ]
    },
    {
      "metadata": {
        "id": "D8-5LDhyaldl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TUl98lj5VbdH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we load the dataset."
      ]
    },
    {
      "metadata": {
        "id": "SQDtwnvNPv7F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "text = open(path_to_file).read()\n",
        "\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V1xLp5hfkwaT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the first 200 characters in text.\n"
      ]
    },
    {
      "metadata": {
        "id": "Z4yKU8mfk25p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(text[:200])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lfz9dW3ek9AL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "metadata": {
        "id": "rIU7FNSJWL7F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Start by building a vocabulary and integerizing the characters."
      ]
    },
    {
      "metadata": {
        "id": "dyxR9mUPPPAe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The unique characters in the file.\n",
        "vocab = sorted(set(text))\n",
        "vocab_size = len(vocab)\n",
        "print ('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "# Creating a mapping from unique characters to indices.\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Show how the first 13 characters from the text are mapped to integers.\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(text[:13], text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aba7ZMWGWVX9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then construct the dataset:"
      ]
    },
    {
      "metadata": {
        "id": "fx6MctSqWU7c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "SEQ_LENGTH = 100\n",
        "\n",
        "# Create training examples / targets\n",
        "chunks = tf.data.Dataset.from_tensor_slices(text_as_int).batch(SEQ_LENGTH+1, drop_remainder=True)\n",
        "  \n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = chunks.map(split_input_target)\n",
        "    \n",
        "# Batch size \n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences, \n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "def one_hot_encode(x):\n",
        "  return tf.keras.utils.to_categorical(x, num_classes=vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s9wQw6zDXaRP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Construct the model\n",
        "\n",
        "Try changing the number of units in the code cell below.\n",
        "\n",
        "We recommend setting the runtime to use GPU to improve performance, but you may change this setting to compare how quickly the training completes without it. To do so, change the variable USE_GPU defined below."
      ]
    },
    {
      "metadata": {
        "id": "YrxeUqffXab1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUM_LSTM_UNITS = [1024]\n",
        "USE_GPU = True\n",
        "DROPOUT_RATE = 0.3\n",
        "\n",
        "assert len(NUM_LSTM_UNITS) > 0\n",
        "\n",
        "def make_model(generate=False):\n",
        "  input_length = 1 if generate else SEQ_LENGTH\n",
        "  batch_size = 1 if generate else BATCH_SIZE\n",
        "  input_size = vocab_size\n",
        "  # We want to save state when generating character by character, but not when\n",
        "  # training in shuffled batches.\n",
        "  stateful = generate\n",
        "  model = tf.keras.Sequential()\n",
        "  if USE_GPU:\n",
        "    lstm_func = tf.keras.layers.CuDNNLSTM\n",
        "  else:\n",
        "    lstm_func = tf.keras.layers.LSTM\n",
        "  batch_input_shape=(batch_size, input_length, input_size)\n",
        "  model.add(lstm_func(units=NUM_LSTM_UNITS[0], \n",
        "                      return_sequences=True,\n",
        "                      stateful=stateful,\n",
        "                      batch_input_shape=batch_input_shape))\n",
        "  if DROPOUT_RATE > 0:\n",
        "      model.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
        "  for layer_index in range(1, len(NUM_LSTM_UNITS)):\n",
        "    model.add(lstm_func(units=NUM_LSTM_UNITS[layer_index],\n",
        "                        return_sequences=True,\n",
        "                        stateful=generate))\n",
        "    if DROPOUT_RATE > 0:\n",
        "      model.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
        "  model.add(tf.keras.layers.Dense(vocab_size, \n",
        "                                  activation=None))\n",
        "  model.build()\n",
        "  return model\n",
        "batch_model = make_model(generate=False)\n",
        "batch_model.summary()\n",
        "gen_model = make_model(generate=True)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "def loss_function(real, preds):\n",
        "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VWwN9J0HwRjM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we define a function to generate text given a seed string as input."
      ]
    },
    {
      "metadata": {
        "id": "22_WCUiLwQVn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Note that it's very important that the model keep state during text generation\n",
        "# since we feed it one character at a time.\n",
        "def generate_text(seed_string='Dog', \n",
        "                  chars_to_generate=300,\n",
        "                  temperature=1.0):\n",
        "  assert len(seed_string) > 0\n",
        "  assert chars_to_generate > 0\n",
        "  gen_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "  gen_model.reset_states()\n",
        "  for c in seed_string:\n",
        "    input_char = one_hot_encode(char2idx[c])\n",
        "    # Add fake batch and sequence dimensions.\n",
        "    input_char = tf.expand_dims(input_char, 0)\n",
        "    input_char = tf.expand_dims(input_char, 0)\n",
        "    predictions = gen_model(input_char)\n",
        "  generated_text = []\n",
        "  for i in xrange(chars_to_generate):\n",
        "    # Drop the sequence dimension from the predictions.\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predicted_id = tf.multinomial(predictions / temperature, num_samples=1)\n",
        "    input_char = tf.expand_dims(one_hot_encode(predicted_id), 0)\n",
        "    predictions = gen_model(input_char)\n",
        "    generated_text.append(idx2char[predicted_id[0,0]])\n",
        "  return ''.join(generated_text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0uSghsUAcajj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Run the following cell to train the model."
      ]
    },
    {
      "metadata": {
        "id": "cg9czi80caSG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "REINITIALIZE_WEIGHTS = True\n",
        "if REINITIALIZE_WEIGHTS:\n",
        "  batch_model = make_model(generate=False)\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "training_losses_per_epoch = []\n",
        "\n",
        "def plot_loss_over_epochs():\n",
        "  plt.ylabel(\"cross entropy loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.title(\"Loss\")\n",
        "  plt.tight_layout()\n",
        "  plt.plot(training_losses_per_epoch)\n",
        "\n",
        "training_start = time.time()\n",
        "print 'Starting to train...'\n",
        "\n",
        "# Training loop.\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    for (batch, (inp, target)) in enumerate(dataset):\n",
        "          with tf.GradientTape() as tape:\n",
        "\n",
        "              one_hot_input = one_hot_encode(inp)\n",
        "              predictions = batch_model(one_hot_input)\n",
        "              loss = loss_function(target, predictions)\n",
        "              \n",
        "          grads = tape.gradient(loss, batch_model.variables)\n",
        "          optimizer.apply_gradients(zip(grads, batch_model.variables))\n",
        "\n",
        "    # Saving (checkpoint) the model every 5 epochs.\n",
        "    if (epoch) % 5 == 0:\n",
        "      batch_model.save_weights(checkpoint_prefix)\n",
        "      \n",
        "    print generate_text()\n",
        "\n",
        "    training_losses_per_epoch.append(loss)\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - epoch_start))\n",
        "\n",
        "    if np.argmin(training_losses_per_epoch) < epoch - 5:\n",
        "      print 'Stopped training early because best loss has not decreased in 5 epochs'\n",
        "      break\n",
        "    \n",
        "print 'Time taken for {} epochs {} sec\\n'.format(epoch + 1, \n",
        "                                                 time.time() - training_start)\n",
        "plot_loss_over_epochs()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "luJMd_AUCOsq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test the model with different start strings\n",
        "\n",
        "Try different inputs in the form below to test the model:"
      ]
    },
    {
      "metadata": {
        "id": "ayD0nqidCT8M",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Generate Strings {run:\"auto\"}\n",
        "seed_string = 'cat'  #@param {type:\"string\"}\n",
        "chars_to_generate = 300 #@param {type:\"integer\"}\n",
        "temperature = 1.0 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "print generate_text(seed_string=seed_string, \n",
        "                    chars_to_generate=chars_to_generate, \n",
        "                    temperature=temperature)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}