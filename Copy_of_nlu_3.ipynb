{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of nlu_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "a6rynpfYl_HY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrywoo/MyML/blob/master/Copy_of_nlu_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "a6rynpfYl_HY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Copyright 2018 Google LLC."
      ]
    },
    {
      "metadata": {
        "id": "3ASrw89Ll5K5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t9vhQgQjmMEn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings by Prediction\n",
        "\n",
        "Please **make a copy** of this Colab notebook before starting this lab. To do so, choose **File**->**Save a copy in Drive**.\n",
        "\n",
        "## Lab Outline\n",
        "  1. Continuous Bag-of-Words (CBOW)\n",
        "  1. The model\n",
        "  1. Parameters and implementation\n",
        "  1. Constructing the model\n",
        "  1. Sampled Softmax\n",
        "  1. Batches\n",
        "  1. Nearest neighbors\n",
        "  1. Scoring\n",
        "  1. Predicting center words\n",
        "  1. Takeaways\n",
        "  1. Experiments"
      ]
    },
    {
      "metadata": {
        "id": "IZkM3-tCX5BM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll discuss the famous `word2vec` algorithm. In contrast to strongly supervised approaches like WordNet or distributional methods like Brown Clustering or SVD, `word2vec` learns vector representations as a by-product of a predictive task.\n",
        "\n",
        "Consider the task of sentence completion (a form of [Cloze test](https://en.wikipedia.org/wiki/Cloze_test)):\n",
        "\n",
        "`the quick brown _____ jumped over the lazy dog`\n",
        "\n",
        "You've probably seen [this sentence](https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog) enough to know the answer is `fox`. At least you know enough about *jumping*, average sizes of *dogs*, the spacial relationship implied by the preposition *over*, and a reasonable idea of what things in the world can be described as both *quick* and *brown* to narrow this down quite a bit. We'd like our model to do the same -- that is, infer a reasonable probability distribution over the words in a vocabulary given this context. In designing this model, we add a bottleneck: words must be represented by fixed-length vectors (embeddings). In order to predict the missing word, our model will need to learn good vectors $\\mathbf{v}_i$ that represent the meaning of this word, and the words around it.\n",
        "\n",
        "We'll describe two closely related models for this task, the Continuous Bag of Words (CBOW) model and the Skip-Gram models (often known as SGNS). Collectively, there are known as `word2vec`, and were originally introduced by:\n",
        "\n",
        "- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf) (Mikolov et al. 2013a)\n",
        "- [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf) (Mikolov et al. 2013b)\n",
        "\n",
        "\n",
        "We'll train these models using gradient descent -- specifically, **backpropagation** as is used to train neural networks. We start with a random set of embeddings, then slowly adjust them to make better predictions. This is similar to training any machine learning model, where  the embeddings are our parameters. As a result, we'll train the `word2vec` models using neural network machinery (Tensorflow) even though the models themselves have just a single layer (the embeddings), and no non-linearities (the neural part of neural networks).\n",
        "\n",
        "### Additional Reading\n",
        "\n",
        "Before (or after) we dig into the details of CBOW and Skip-Gram below, you may want to read more about word embeddings. In addition to the `word2vec` papers above, you may find the following helpful:\n",
        "\n",
        "- [Deep Learning, NLP, and Representations](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/) (Chris Olah, 2014)\n",
        "- [TensorFlow Projector](http://projector.tensorflow.org/) to visualize `word2vec` in your browser.\n",
        "- [GloVe](https://nlp.stanford.edu/projects/glove/), another popular method of creating embeddings that bridges the gap between `word2vec` and SVD.\n"
      ]
    },
    {
      "metadata": {
        "id": "e_YY8DGamV4X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Let's get started with the imports needed for the rest of this lab.\n",
        "\n",
        "We'll also need to load some utility functions which will help us work with the corpus."
      ]
    },
    {
      "metadata": {
        "id": "1ms9gn4nmnkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports\n",
        "\n",
        "Run this code cell to add in all of the imports."
      ]
    },
    {
      "metadata": {
        "id": "D9HsO0UVmoGY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "import time\n",
        "import nltk\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "keGz4qerpvYG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### bAbI task corpus reader\n",
        "\n",
        "Start by running the cell below to load the corpus reader."
      ]
    },
    {
      "metadata": {
        "id": "DMAycq3FpwFn",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title bAbI Task corpus reader\n",
        "\n",
        "import sys, os\n",
        "import glob\n",
        "import re\n",
        "from collections import namedtuple\n",
        "\n",
        "# Struct types for different lines in the bAbI dataset.\n",
        "# StoryLine represents \"ID text\" lines as (int, string)\n",
        "# QALine represents \"ID question answer support\" lines as\n",
        "# (int, string, string, list(int)).\n",
        "# If tokenized, string fields can be replaced with list(string).\n",
        "StoryLine = namedtuple(\"StoryLine\", [\"id\", \"text\"])\n",
        "QALine = namedtuple(\"QALine\", [\"id\", \"question\", \"answer\", \"support_ids\"])\n",
        "\n",
        "class BabiTaskCorpusReader(object):\n",
        "    \"\"\"Corpus reader for the bAbI tasks dataset.\n",
        "\n",
        "    See https://research.fb.com/downloads/babi/ for details.\n",
        "\n",
        "    This class exposes a similar interface to NLTK's corpus readers, and should\n",
        "    be interchangable with them in many applications.\n",
        "\n",
        "    Example usage:\n",
        "\n",
        "    import babi_utils\n",
        "    import nltk\n",
        "    tok = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
        "    cr = babi_utils.BabiTaskCorpusReader(\"/home/babi/en\",\n",
        "                                         tokenizer=tok.tokenize)\n",
        "    words = list(cr.words())\n",
        "    print words[:8]\n",
        "    # ['John', 'travelled', 'to', 'the', 'hallway', '.', 'Mary', 'journeyed']\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ALL_FILES = [\n",
        "        'qa10_indefinite-knowledge_test.txt',\n",
        "        'qa10_indefinite-knowledge_train.txt',\n",
        "        'qa11_basic-coreference_test.txt',\n",
        "        'qa11_basic-coreference_train.txt',\n",
        "        'qa12_conjunction_test.txt',\n",
        "        'qa12_conjunction_train.txt',\n",
        "        'qa13_compound-coreference_test.txt',\n",
        "        'qa13_compound-coreference_train.txt',\n",
        "        'qa14_time-reasoning_test.txt',\n",
        "        'qa14_time-reasoning_train.txt',\n",
        "        'qa15_basic-deduction_test.txt',\n",
        "        'qa15_basic-deduction_train.txt',\n",
        "        'qa16_basic-induction_test.txt',\n",
        "        'qa16_basic-induction_train.txt',\n",
        "        'qa17_positional-reasoning_test.txt',\n",
        "        'qa17_positional-reasoning_train.txt',\n",
        "        'qa18_size-reasoning_test.txt',\n",
        "        'qa18_size-reasoning_train.txt',\n",
        "        'qa19_path-finding_test.txt',\n",
        "        'qa19_path-finding_train.txt',\n",
        "        'qa1_single-supporting-fact_test.txt',\n",
        "        'qa1_single-supporting-fact_train.txt',\n",
        "        'qa20_agents-motivations_test.txt',\n",
        "        'qa20_agents-motivations_train.txt',\n",
        "        'qa2_two-supporting-facts_test.txt',\n",
        "        'qa2_two-supporting-facts_train.txt',\n",
        "        'qa3_three-supporting-facts_test.txt',\n",
        "        'qa3_three-supporting-facts_train.txt',\n",
        "        'qa4_two-arg-relations_test.txt',\n",
        "        'qa4_two-arg-relations_train.txt',\n",
        "        'qa5_three-arg-relations_test.txt',\n",
        "        'qa5_three-arg-relations_train.txt',\n",
        "        'qa6_yes-no-questions_test.txt',\n",
        "        'qa6_yes-no-questions_train.txt',\n",
        "        'qa7_counting_test.txt',\n",
        "        'qa7_counting_train.txt',\n",
        "        'qa8_lists-sets_test.txt',\n",
        "        'qa8_lists-sets_train.txt',\n",
        "        'qa9_simple-negation_test.txt',\n",
        "        'qa9_simple-negation_train.txt'\n",
        "    ]\n",
        "\n",
        "    def __init__(self, directory, mask=\"qa*.txt\",\n",
        "                 file_list=ALL_FILES,\n",
        "                 file_reader=open,\n",
        "                 tokenizer=lambda s: s.split(),\n",
        "                 verbose=False):\n",
        "        \"\"\"Construct a corpus reader for the bAbI tasks dataset.\n",
        "\n",
        "        Args:\n",
        "            directory: (string) path to bAbI text files (e.g. /home/babi/en/)\n",
        "            mask: (string) file glob to match particular files. Use\n",
        "                \"qa16_*\" e.g. to match task 16.\n",
        "            file_list: (list(string) or None) If None, will glob directory to\n",
        "                find files. Otherwise, will use the given list of basenames.\n",
        "            file_reader: (function string -> fd) optional replacement for\n",
        "                Python's built-in open(...) method, to be used for reading\n",
        "                from alternative file-like objects.\n",
        "            tokenizer: function string -> list(string), used to split\n",
        "                sentences.\n",
        "            verbose: (bool) if true, will print when reading files.\n",
        "        \"\"\"\n",
        "        self._open = file_reader\n",
        "        self._tokenizer = tokenizer\n",
        "        self._verbose = verbose\n",
        "\n",
        "        if file_list:\n",
        "            basenames = glob.fnmatch.filter(file_list, mask)\n",
        "            filenames = [os.path.join(directory, f) for f in basenames]\n",
        "        else:\n",
        "            # Glob directory\n",
        "            pattern = os.path.join(directory, mask)\n",
        "            filenames = glob.glob(pattern)\n",
        "\n",
        "        # Filenames of form qaXX_task-name_train.txt\n",
        "        # Want to sort by XX as a number\n",
        "        key_fn = lambda f: (int(os.path.basename(f).split(\"_\")[0][2:]), f)\n",
        "        self._filenames = sorted(filenames, key=key_fn)\n",
        "        # Filenames should be nonempty!\n",
        "        assert(self._filenames), \"No files found matching [{:s}]\".format(mask)\n",
        "\n",
        "    def filenames(self):\n",
        "        return self._filenames\n",
        "\n",
        "    def parse_line(self, line):\n",
        "        \"\"\"Parse a single line from the bAbI corpus.\n",
        "\n",
        "        Line is of one of the two forms:\n",
        "        ID text\n",
        "        ID question[tab]answer[tab]supporting fact IDs\n",
        "\n",
        "        See https://research.fb.com/downloads/babi/\n",
        "\n",
        "        Args:\n",
        "            line: (string)\n",
        "\n",
        "        Returns:\n",
        "            (id, text) as (int, string)\n",
        "            OR (id, question, answer, [ids]) as (int, string, string, list(int))\n",
        "        \"\"\"\n",
        "        id_text, rest = line.split(\" \", 1)\n",
        "        id = int(id_text)\n",
        "        if \"\\t\" in rest:\n",
        "            question, answer, s_ids_text = rest.split(\"\\t\")\n",
        "            s_ids = map(int, s_ids_text.split())\n",
        "            return QALine(id, question.strip(), answer.strip(), s_ids)\n",
        "        else:\n",
        "            return StoryLine(id, rest.strip())\n",
        "\n",
        "    def tokenize_parsed_line(self, line):\n",
        "        if isinstance(line, StoryLine):\n",
        "            return StoryLine(line.id, self._tokenizer(line.text))\n",
        "        else:\n",
        "            return QALine(line.id,\n",
        "                          self._tokenizer(line.question),\n",
        "                          self._tokenizer(line.answer),\n",
        "                          line.support_ids)\n",
        "\n",
        "    def _line_iterator(self):\n",
        "        for f in self._filenames:\n",
        "            if self._verbose:\n",
        "                print >> sys.stderr, \"Reading {:s}\".format(os.path.basename(f)),\n",
        "            with self._open(f) as fd:\n",
        "                for line in fd:\n",
        "                    yield line.strip()\n",
        "            if self._verbose:\n",
        "                print >> sys.stderr, \"...done!\"\n",
        "\n",
        "    def examples(self, tokenize=True):\n",
        "        \"\"\"Iterator over complete stories (training examples).\n",
        "\n",
        "        A story spans multiple lines, of the form:\n",
        "\n",
        "        1 text one\n",
        "        2 text two\n",
        "        3 text three\n",
        "        4 question[tab]answer[tab]supporting fact IDs\n",
        "\n",
        "        Args:\n",
        "            tokenize: (bool) If true, will tokenize text fields.\n",
        "\n",
        "        Returns:\n",
        "            iterator yielding list(StoryLine|QALine)\n",
        "              if tokenize=True, then text, question, and answer will be\n",
        "              list(string); otherwise they will be plain strings.\n",
        "        \"\"\"\n",
        "        buffer = []\n",
        "        for line in self._line_iterator():\n",
        "            parsed = self.parse_line(line)\n",
        "            if tokenize:\n",
        "                parsed = self.tokenize_parsed_line(parsed)\n",
        "            # If new story item, flush buffer.\n",
        "            if buffer and parsed.id <= buffer[-1].id:\n",
        "                yield buffer\n",
        "                buffer = []\n",
        "            buffer.append(parsed)\n",
        "        # Flush at end.\n",
        "        yield buffer\n",
        "        buffer = []\n",
        "\n",
        "    def _raw_sents_impl(self, stories=False, questions=False, answers=False):\n",
        "        for line in self._line_iterator():\n",
        "            parsed = self.parse_line(line)\n",
        "            if isinstance(parsed, StoryLine) and stories:\n",
        "                yield parsed.text\n",
        "            else:\n",
        "                if questions:\n",
        "                    yield parsed.question\n",
        "                if answers:\n",
        "                    yield parsed.answer\n",
        "\n",
        "    def raw_sents(self):\n",
        "        \"\"\"Iterator over utterances in the corpus.\n",
        "\n",
        "        Returns untokenized sentences.\n",
        "\n",
        "        Returns:\n",
        "            iterator yielding string\n",
        "        \"\"\"\n",
        "        return self._raw_sents_impl(stories=True,\n",
        "                                    questions=True,\n",
        "                                    answers=True)\n",
        "\n",
        "    def sents(self):\n",
        "        \"\"\"Iterator over utterances in the corpus.\n",
        "\n",
        "        Returns tokenized sentences, a la NLTK.\n",
        "\n",
        "        Returns:\n",
        "            iterator yielding list(string)\n",
        "        \"\"\"\n",
        "        for sentence in self.raw_sents():\n",
        "            yield self._tokenizer(sentence)\n",
        "\n",
        "\n",
        "    def words(self):\n",
        "        \"\"\"Iterator over words in the corpus.\n",
        "\n",
        "        Returns:\n",
        "            iterator yielding string\n",
        "        \"\"\"\n",
        "        for sentence in self.sents():\n",
        "            for word in sentence:\n",
        "                yield word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Er70vvqXqSbQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Corpus utilities\n",
        "\n",
        "Next, let's load some utility functions which will help us work with the corpus."
      ]
    },
    {
      "metadata": {
        "id": "NFZPBsrhqXdn",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Vocabulary helper functions\n",
        "\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "\n",
        "class Vocabulary(object):\n",
        "\n",
        "  START_TOKEN = u\"<s>\"\n",
        "  END_TOKEN   = u\"</s>\"\n",
        "  UNK_TOKEN   = u\"<unk>\"\n",
        "\n",
        "  def __init__(self, tokens, size=None):\n",
        "    \"\"\"Create a Vocabulary object.\n",
        "\n",
        "    Args:\n",
        "        tokens: iterator( string )\n",
        "        size: None for unlimited, or int > 0 for a fixed-size vocab.\n",
        "              Vocabulary size includes special tokens <s>, </s>, and <unk>\n",
        "    \"\"\"\n",
        "    self.unigram_counts = collections.Counter(tokens)\n",
        "    self.bigram_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "    word1 = None\n",
        "    for word in tokens:\n",
        "        if word1 is None:\n",
        "            pass\n",
        "        self.bigram_counts[word1][word] += 1\n",
        "        word1 = word\n",
        "    self.bigram_counts.default_factory = None  # make into a normal dict\n",
        "\n",
        "    # Leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
        "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 3))\n",
        "    vocab = ([self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN] +\n",
        "             [w for w,c in top_counts])\n",
        "\n",
        "    # Assign an id to each word, by frequency\n",
        "    self.id_to_word = dict(enumerate(vocab))\n",
        "    self.word_to_id = {v:k for k,v in self.id_to_word.iteritems()}\n",
        "    self.size = len(self.id_to_word)\n",
        "    if size is not None:\n",
        "        assert(self.size <= size)\n",
        "\n",
        "    # For convenience\n",
        "    self.wordset = set(self.word_to_id.iterkeys())\n",
        "\n",
        "    # Store special IDs\n",
        "    self.START_ID = self.word_to_id[self.START_TOKEN]\n",
        "    self.END_ID = self.word_to_id[self.END_TOKEN]\n",
        "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
        "\n",
        "  def words_to_ids(self, words):\n",
        "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
        "\n",
        "  def ids_to_words(self, ids):\n",
        "    return [self.id_to_word[i] for i in ids]\n",
        "\n",
        "  def pad_sentence(self, words, use_eos=True):\n",
        "    ret = [self.START_TOKEN] + words\n",
        "    if use_eos:\n",
        "      ret.append(self.END_TOKEN)\n",
        "    return ret\n",
        "\n",
        "  def sentence_to_ids(self, words, use_eos=True):\n",
        "    return self.words_to_ids(self.pad_sentence(words, use_eos))\n",
        "\n",
        "  def ordered_words(self):\n",
        "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
        "    return self.ids_to_words(range(self.size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VWL-gkmgqT1f",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Utilities\n",
        "\n",
        "import re\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "# For pretty-printing\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "UNK_TOKEN   = u\"<unk>\"\n",
        "\n",
        "def flatten(list_of_lists):\n",
        "    \"\"\"Flatten a list-of-lists into a single list.\"\"\"\n",
        "    return list(itertools.chain.from_iterable(list_of_lists))\n",
        "\n",
        "def pretty_print_matrix(M, rows=None, cols=None, dtype=float, float_fmt=\"{0:.04f}\"):\n",
        "    \"\"\"Pretty-print a matrix using Pandas.\n",
        "\n",
        "    Args:\n",
        "      M : 2D numpy array\n",
        "      rows : list of row labels\n",
        "      cols : list of column labels\n",
        "      dtype : data type (float or int)\n",
        "      float_fmt : format specifier for floats\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(M, index=rows, columns=cols, dtype=dtype)\n",
        "    old_fmt_fn = pd.get_option('float_format')\n",
        "    pd.set_option('float_format', lambda f: float_fmt.format(f))\n",
        "    display(df)\n",
        "    pd.set_option('float_format', old_fmt_fn)  # reset Pandas formatting\n",
        "\n",
        "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
        "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
        "    since = since or time.time()\n",
        "    until = until or time.time()\n",
        "    delta_s = until - since\n",
        "    hours, remainder = divmod(delta_s, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    return fmt % (hours, minutes, seconds)\n",
        "\n",
        "\n",
        "##\n",
        "# Word processing functions\n",
        "def canonicalize_digits(word):\n",
        "    if any([c.isalpha() for c in word]): return word\n",
        "    word = re.sub(\"\\d\", \"DG\", word)\n",
        "    if word.startswith(\"DG\"):\n",
        "        word = word.replace(\",\", \"\") # remove thousands separator\n",
        "    return word\n",
        "\n",
        "def canonicalize_word(word, wordset=None, digits=True):\n",
        "    word = word.lower()\n",
        "    if digits:\n",
        "        if (wordset != None) and (word in wordset): return word\n",
        "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
        "    if (wordset == None) or (word in wordset):\n",
        "        return word\n",
        "    else:\n",
        "        return UNK_TOKEN\n",
        "\n",
        "def canonicalize_words(words, **kw):\n",
        "    return [canonicalize_word(word, **kw) for word in words]\n",
        "\n",
        "##\n",
        "# Data loading functions\n",
        "def get_corpus(name=\"brown\"):\n",
        "    import nltk\n",
        "    assert(nltk.download(name))\n",
        "    return nltk.corpus.__getattr__(name)\n",
        "\n",
        "def build_vocab(corpus, V=10000):\n",
        "    token_feed = (canonicalize_word(w) for w in corpus.words())\n",
        "    vocab = Vocabulary(token_feed, size=V)\n",
        "    return vocab\n",
        "\n",
        "def get_train_test_sents(corpus, split=0.8, shuffle=True):\n",
        "    \"\"\"Generate train/test split for unsupervised tasks.\n",
        "\n",
        "    Args:\n",
        "      corpus: nltk.corpus that supports sents() function\n",
        "      split (double): fraction to use as training set\n",
        "      shuffle (int or bool): seed for shuffle of input data, or False to just\n",
        "      take the training data as the first xx% contiguously.\n",
        "\n",
        "    Returns:\n",
        "      train_sentences, test_sentences ( list(list(string)) ): the train and test\n",
        "      splits\n",
        "    \"\"\"\n",
        "    sentences = np.array(list(corpus.sents()), dtype=object)\n",
        "    fmt = (len(sentences), sum(map(len, sentences)))\n",
        "    print \"Loaded {:,} sentences ({:g} tokens)\".format(*fmt)\n",
        "\n",
        "    if shuffle:\n",
        "        rng = np.random.RandomState(shuffle)\n",
        "        rng.shuffle(sentences)  # in-place\n",
        "    train_frac = 0.8\n",
        "    split_idx = int(train_frac * len(sentences))\n",
        "    train_sentences = sentences[:split_idx]\n",
        "    test_sentences = sentences[split_idx:]\n",
        "\n",
        "    fmt = (len(train_sentences), sum(map(len, train_sentences)))\n",
        "    print \"Training set: {:,} sentences ({:,} tokens)\".format(*fmt)\n",
        "    fmt = (len(test_sentences), sum(map(len, test_sentences)))\n",
        "    print \"Test set: {:,} sentences ({:,} tokens)\".format(*fmt)\n",
        "\n",
        "    return train_sentences, test_sentences\n",
        "\n",
        "def preprocess_sentences(sentences, vocab, use_eos=False, emit_ids=True):\n",
        "    \"\"\"Preprocess sentences by canonicalizing and mapping to ids.\n",
        "\n",
        "    Args:\n",
        "      sentences ( list(list(string)) ): input sentences\n",
        "      vocab: Vocabulary object, already initialized\n",
        "      use_eos: if true, will add </s> token to end of sentence.\n",
        "      emit_ids: if true, will emit as ids. Otherwise, will be preprocessed\n",
        "          tokens.\n",
        "\n",
        "    Returns:\n",
        "      ids ( array(int) ): flattened array of sentences, including boundary <s>\n",
        "      tokens.\n",
        "    \"\"\"\n",
        "    # Add sentence boundaries, canonicalize, and handle unknowns\n",
        "    word_preproc = lambda w: canonicalize_word(w, wordset=vocab.word_to_id)\n",
        "    ret = []\n",
        "    for s in sentences:\n",
        "        canonical_words = vocab.pad_sentence(map(word_preproc, s),\n",
        "                                             use_eos=use_eos)\n",
        "        ret.extend(vocab.words_to_ids(canonical_words) if emit_ids else\n",
        "                   canonical_words)\n",
        "    if not use_eos:  # add additional <s> to end if needed\n",
        "        ret.append(vocab.START_ID if emit_ids else vocab.START_TOKEN)\n",
        "    return np.array(ret, dtype=(np.int32 if emit_ids else object))\n",
        "\n",
        "\n",
        "def load_corpus(corpus, split=0.8, V=10000, shuffle=0):\n",
        "    \"\"\"Load a named corpus and split train/test along sentences.\n",
        "\n",
        "    This is a convenience wrapper to chain together several functions from this\n",
        "    module, and produce a train/test split suitable for input to most models.\n",
        "\n",
        "    Sentences are preprocessed by canonicalization and converted to ids\n",
        "    according to the constructed vocabulary, and interspersed with <s> tokens\n",
        "    to denote sentence bounaries.\n",
        "\n",
        "    Args:\n",
        "        corpus: (string | corpus reader) If a string, will fetch the\n",
        "            NLTK corpus of that name.\n",
        "        split: (float \\in (0,1]) fraction of examples in train split\n",
        "        V: (int) vocabulary size (including special tokens)\n",
        "        shuffle: (int) if > 0, use as random seed to shuffle sentence prior to\n",
        "            split. Can change this to get different splits.\n",
        "\n",
        "    Returns:\n",
        "        (vocab, train_ids, test_ids)\n",
        "        vocab: vocabulary.Vocabulary object\n",
        "        train_ids: flat (1D) np.array(int) of ids\n",
        "        test_ids: flat (1D) np.array(int) of ids\n",
        "    \"\"\"\n",
        "    if isinstance(corpus, str):\n",
        "        corpus = get_corpus(corpus)\n",
        "    vocab = build_vocab(corpus, V)\n",
        "    train_sentences, test_sentences = get_train_test_sents(corpus, split, shuffle)\n",
        "    train_ids = preprocess_sentences(train_sentences, vocab)\n",
        "    test_ids = preprocess_sentences(test_sentences, vocab)\n",
        "    return vocab, train_ids, test_ids\n",
        "\n",
        "##\n",
        "# Window and batch functions\n",
        "def rnnlm_batch_generator(ids, batch_size, max_time):\n",
        "    \"\"\"Convert ids to data-matrix form for RNN language modeling.\"\"\"\n",
        "    # Clip to multiple of max_time for convenience\n",
        "    clip_len = ((len(ids)-1) / batch_size) * batch_size\n",
        "    input_w = ids[:clip_len]     # current word\n",
        "    target_y = ids[1:clip_len+1]  # next word\n",
        "    # Reshape so we can select columns\n",
        "    input_w = input_w.reshape([batch_size,-1])\n",
        "    target_y = target_y.reshape([batch_size,-1])\n",
        "\n",
        "    # Yield batches\n",
        "    for i in xrange(0, input_w.shape[1], max_time):\n",
        "        yield input_w[:,i:i+max_time], target_y[:,i:i+max_time]\n",
        "\n",
        "\n",
        "def build_windows(ids, N, shuffle=True):\n",
        "    \"\"\"Build window input to the window model.\n",
        "\n",
        "    Takes a sequence of ids, and returns a data matrix where each row\n",
        "    is a window and target for the window model. For N=3:\n",
        "        windows[i] = [w_3, w_2, w_1, w_0]\n",
        "\n",
        "    For language modeling, N is the context size and you can use y = windows[:,-1]\n",
        "    as the target words and x = windows[:,:-1] as the contexts.\n",
        "\n",
        "    For CBOW, N is the window size and you can use y = windows[:,N/2] as the target words\n",
        "    and x = np.hstack([windows[:,:N/2], windows[:,:N/2+1]]) as the contexts.\n",
        "\n",
        "    For skip-gram, you can use x = windows[:,N/2] as the input words and y = windows[:,i]\n",
        "    where i != N/2 as the target words.\n",
        "\n",
        "    Args:\n",
        "      ids: np.array(int32) of input ids\n",
        "      shuffle: if true, will randomly shuffle the rows\n",
        "\n",
        "    Returns:\n",
        "      windows: np.array(int32) of shape [len(ids)-N, N+1]\n",
        "        i.e. each row is a window, of length N+1\n",
        "    \"\"\"\n",
        "    windows = np.zeros((len(ids)-N, N+1), dtype=int)\n",
        "    for i in xrange(N+1):\n",
        "        # First column: first word, etc.\n",
        "        windows[:,i] = ids[i:len(ids)-(N-i)]\n",
        "    if shuffle:\n",
        "        # Shuffle rows\n",
        "        np.random.shuffle(windows)\n",
        "    return windows\n",
        "\n",
        "\n",
        "def batch_generator(data, batch_size):\n",
        "    \"\"\"Generate minibatches from data.\n",
        "\n",
        "    Args:\n",
        "      data: array-like, supporting slicing along first dimension\n",
        "      batch_size: int, batch size\n",
        "\n",
        "    Yields:\n",
        "      minibatches of maximum size batch_size\n",
        "    \"\"\"\n",
        "    for i in xrange(0, len(data), batch_size):\n",
        "        yield data[i:i+batch_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "23Ue47nbqaYS",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title TSV Corpus Reader\n",
        "\n",
        "import sys, os\n",
        "\n",
        "class TSVCorpusReader(object):\n",
        "    \"\"\"Corpus reader for TSV files.\n",
        "\n",
        "    Input files are assumed to contain one sentence per line, with tokens\n",
        "    separated by tabs:\n",
        "\n",
        "    foo[tab]bar[tab]baz\n",
        "    span[tab]eggs\n",
        "\n",
        "    Would correspond to the two-sentence corpus:\n",
        "        [\"foo\", \"bar\", \"baz\"],\n",
        "        [\"spam\", \"eggs\"]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sentence_file, preload=True, file_reader=open):\n",
        "        \"\"\"Construct a corpus reader for the given file.\n",
        "\n",
        "        Args:\n",
        "            sentence_file: (string) path to a TSV file with one sentence per\n",
        "                line.\n",
        "            preload: (bool) If true, will read entire corpus to memory on\n",
        "                construction. Otherwise, will load on-demand.\n",
        "            file_reader: (function string -> fd) optional replacement for\n",
        "                Python's built-in open(...) method, to be used for reading\n",
        "                from alternative file-like objects.\n",
        "        \"\"\"\n",
        "        self._open = file_reader\n",
        "        self._sentence_file = sentence_file\n",
        "        self._sentence_cache = []\n",
        "\n",
        "        if preload:\n",
        "            self._sentence_cache = list(self.sents())\n",
        "\n",
        "    def _line_iterator(self):\n",
        "        with self._open(self._sentence_file) as fd:\n",
        "            for line in fd:\n",
        "                yield line.strip()\n",
        "\n",
        "    def sents(self):\n",
        "        \"\"\"Iterator over sentences in the corpus.\n",
        "\n",
        "        Yields:\n",
        "            list(string) of tokens\n",
        "        \"\"\"\n",
        "        if self._sentence_cache:\n",
        "            for sentence in self._sentence_cache:\n",
        "                yield sentence\n",
        "        else:\n",
        "            # If no cache, actually read the file.\n",
        "            for line in self._line_iterator():\n",
        "                yield line.split(\"\\t\")\n",
        "\n",
        "    def words(self):\n",
        "        \"\"\"Iterator over words in the corpus.\n",
        "\n",
        "        Yields:\n",
        "            (string) tokens\n",
        "        \"\"\"\n",
        "        for sentence in self.sents():\n",
        "            for word in sentence:\n",
        "                yield word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QMOx2ZSXp6ge",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading the Corpus\n",
        "\n",
        "We are going to be training on the [bAbI Tasks corpus](https://research.fb.com/downloads/babi/). This is an artificial corpus of around 1M words, which has the advantage of looking like natural language while being free from noise and having a very small vocabulary. This means we'll see each word many times, and so will have plenty of signal to get useful embeddings.\n",
        "\n",
        "We'll pre-process the inputs, lowercasing and canonicalizing digits and adding `<s>` and `</s>` markers to sentence boundaries. This is done internally by the `load_corpus` function provided with utilities - for now, don't worry too much about how it works. The form we'll deal with is a list of ids: `train_ids` for the training set, and `test_ids` for a held-out development set."
      ]
    },
    {
      "metadata": {
        "id": "WNL1zYYMqDu2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/mledu-datasets/babi_tasks_1-20_v1-2.tar.gz -O /tmp/babi_tasks_1-20_v1-2.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fJYhq3kpqEVv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "\n",
        "local_tar = '/tmp/babi_tasks_1-20_v1-2.tar.gz'\n",
        "tar_ref = tarfile.open(local_tar, 'r:gz')\n",
        "tar_ref.extractall('/tmp')\n",
        "tar_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VJ83MFxhwfdQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note: running the following cell to load the corpus and split into training and test sets may take 20-30 seconds."
      ]
    },
    {
      "metadata": {
        "id": "o20rmw2XqGUX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print 'Loading bAbI corpus... ',\n",
        "babi_corpus = BabiTaskCorpusReader(\n",
        "    \"/tmp/tasks_1-20_v1-2/en\",\n",
        "    tokenizer=nltk.tokenize.treebank.TreebankWordTokenizer().tokenize)\n",
        "print 'Done'\n",
        "\n",
        "# Load dataset and split into train and test.\n",
        "corpus = babi_corpus \n",
        "vocab, train_ids, test_ids = load_corpus(\n",
        "    corpus, split=0.8, V=10000, shuffle=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mqF2lfUjqlAB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Continuous Bag-of-Words (CBOW)\n",
        "\n",
        "The goal here is to solve the fill-in-the-blank task directly. We treat it as a supervised learning task, where the inputs are the surrounding words (the **context**), and the **target** is the center word:\n",
        "\n",
        "$$ \\begin{eqnarray}\n",
        "x & = & \\{\\texttt{quick},\\ \\texttt{brown},\\ \\texttt{jumped},\\ \\texttt{over}\\} \\\\\n",
        "y & = & \\ \\texttt{fox}\n",
        "\\end{eqnarray} $$\n",
        "\n",
        "We're interested in real-valued vectors, so let's pick a dimension $d$ (typically, $d = 100$ or so) and assign each word $i \\in V$ in our vocabulary to a vector $\\mathbf{w}_i \\in \\mathbb{R}^d$. Now we have embedded inputs, which look like:\n",
        "\n",
        "$$ \\begin{eqnarray}\n",
        "\\mathbf{x} & = & \\{\\mathbf{w}_\\texttt{quick},\n",
        "                 \\ \\mathbf{w}_\\texttt{brown},\n",
        "                 \\ \\mathbf{w}_\\texttt{jumped},\n",
        "                 \\ \\mathbf{w}_\\texttt{over}\\} \\\\\n",
        "\\mathbf{y} & = & \\ \\mathbf{u}_\\texttt{fox}\n",
        "\\end{eqnarray} $$\n",
        "\n",
        "and our goal is to predict $\\mathbf{y}$ given $\\mathbf{x}$. Note that we gave the target word a vector $\\mathbf{u}_i$ denoted by a different letter - we'll see why this is below.\n",
        "\n",
        "We can think of these embedding vectors a bit like feature vectors. In a classical modeling framework, we might have inputs $\\mathbf{x}_i \\in \\mathbb{R}^k$, where each vector dimension $j = 1,\\ldots,k$ represents a feature and we learn some parameters $\\mathbf{\\theta} \\in \\mathbb{R}^k$ to fit them. The key difference with `word2vec` is that instead, the features $\\mathbf{w}_i$ won't be fixed -- their values are what we are trying to learn."
      ]
    },
    {
      "metadata": {
        "id": "nmZqkirUqs0Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The model\n",
        "\n",
        "What is the actual form of this model, of our prediction function?\n",
        "\n",
        "Let's take one step back, and let our input $x \\subseteq V$ be a set of indices representing words. _(As with SVD embeddings, the actual indices are arbitrary - they need only be unique and consistent.)_ \n",
        "\n",
        "So $ x = \\{\\texttt{quick},\\ \\texttt{brown},\\ \\texttt{jumped},\\ \\texttt{over}\\}$ might be represented as `{7, 42, 3, 127}`. We'll embed these by taking the vector representations (remember, these are model parameters and not fixed yet!), to get:\n",
        "\n",
        "$$\\mathbf{x} = \\{\\mathbf{w}_\\texttt{quick},\n",
        "               \\ \\mathbf{w}_\\texttt{brown},\n",
        "               \\ \\mathbf{w}_\\texttt{jumped},\n",
        "               \\ \\mathbf{w}_\\texttt{over}\\} \n",
        "             = \\{\\mathbf{w}_{7},\n",
        "               \\ \\mathbf{w}_{42},\n",
        "               \\ \\mathbf{w}_{3},\n",
        "               \\ \\mathbf{w}_{127}\\} \n",
        "$$\n",
        "\n",
        "Now we'll take these embeddings and add them together to get a single vector $\\mathbf{\\bar{w}} \\in \\mathbb{R}^d$:\n",
        "\n",
        "$$ \\mathbf{\\bar{w}} = \\sum_{i \\in x} \\mathbf{w}_i = \\mathbf{w}_{7} + \\mathbf{w}_{42} + \\mathbf{w}_{3} + \\mathbf{w}_{127} $$\n",
        "\n",
        "This is the origin of the name: \"bag-of-words\" because we're just adding the word representations together while ignoring position and order, and \"continuous\" because we're doing this on real-valued (continuous) vectors.\n",
        "\n",
        "And now we'll take the dot product of this with vectors for _each possible word_ $j \\in V$, add a bias term, and feed the result through a softmax to predict a probability that each word $j$ is the center word:\n",
        "\n",
        "$$ \\begin{eqnarray}\n",
        "a_j & = & \\mathbf{\\bar{w}}\\cdot \\mathbf{u}_j + b_j \\\\\n",
        "\\mathbf{\\hat{y}} & = & \\text{softmax}(\\mathbf{a})\n",
        "\\end{eqnarray} $$\n",
        "\n",
        "where $\\mathbf{a} \\in \\mathbb{R}^V$ are logits and $\\mathbf{\\hat{y}} \\in [0,1]^V$ is a vector of our predicted probabilities ($\\sum_{j} \\hat{y}_j = 1$).\n",
        "\n",
        "This last step should be very familiar - it's just logistic regression! Specifically, if we think of $\\mathbf{\\bar{w}}$ as a fixed set of features, then the CBOW model is just multiclass logistic regression with $V$ classes, one for each word in the vocabulary. The learned parameters are the \"output vectors\" $\\mathbf{u}_j$, a weight for each feature, for each class."
      ]
    },
    {
      "metadata": {
        "id": "fB2nCQHLq8SQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parameters and implementation\n",
        "\n",
        "Recall that our goal is to learn the word vectors $\\mathbf{w}_i$ (and $\\mathbf{u}_i$). For convenience, let's stack them into two matrices, which we'll call $W$ and $U$:\n",
        "\n",
        "$$ W = \\begin{pmatrix} \n",
        "\\vcenter{\\Rule{15px}{0.5px}{1px}}\\ \\mathbf{w}_1\\ \\vcenter{\\Rule{15px}{0.5px}{1px}}\\\\ \n",
        "\\vcenter{\\Rule{15px}{0.5px}{1px}}\\ \\mathbf{w}_2\\ \\vcenter{\\Rule{15px}{0.5px}{1px}}\\\\ \n",
        "\\vdots \\\\\n",
        "\\vcenter{\\Rule{15px}{0.5px}{1px}}\\ \\mathbf{w}_V\\ \\vcenter{\\Rule{15px}{0.5px}{1px}} \n",
        "\\end{pmatrix} \\ \\in \\mathbb{R}^{V \\times d} \n",
        "\\quad \\text{and} \\quad\n",
        "U = \\begin{pmatrix} \n",
        "\\vcenter{\\Rule{15px}{0.5px}{1px}}\\ \\mathbf{u}_1\\ \\vcenter{\\Rule{15px}{0.5px}{1px}}\\\\ \n",
        "\\vcenter{\\Rule{15px}{0.5px}{1px}}\\ \\mathbf{u}_2\\ \\vcenter{\\Rule{15px}{0.5px}{1px}}\\\\ \n",
        "\\vdots \\\\\n",
        "\\vcenter{\\Rule{15px}{0.5px}{1px}}\\ \\mathbf{u}_V\\ \\vcenter{\\Rule{15px}{0.5px}{1px}} \n",
        "\\end{pmatrix} \\ \\in \\mathbb{R}^{V \\times d} \n",
        "$$\n",
        "\n",
        "We'll refer to $W$ as the input matrix (and its rows the input embeddings), and $U$ as the output matrix (and its rows the output embeddings). We'll also have one additional parameter, the biases $\\mathbf{b} \\in \\mathbb{R}^V$.\n",
        "\n",
        "_**Note:** Keeping two separate sets of vectors makes the model easier to train, but there isn't a major distinction otherwise - commonly, we can just concatenate them when we're done to get word embeddings $\\mathbf{v}_i = [\\mathbf{w}_i, \\mathbf{u}_i]$._\n",
        "\n",
        "Now we can write our model in matrix form, interpreting $x$ as a many-hot vector $\\mathbf{x} \\in \\mathbb{R}^V$ where \n",
        "$$ x_i = \\begin{cases}\n",
        "1\\ \\text{if}\\ i \\in x \\\\\n",
        "0\\ \\text{otherwise}\n",
        "\\end{cases} $$\n",
        "\n",
        "So we have for our final model equations:\n",
        "$$\\begin{eqnarray}\n",
        "\\mathbf{\\bar{w}} & = & W \\mathbf{x} \\\\ \n",
        "\\mathbf{a} & = & U \\mathbf{\\bar{w}} + \\mathbf{b} \\\\\n",
        "\\mathbf{\\hat{y}} & = & \\text{softmax}(\\mathbf{a})\n",
        "\\end{eqnarray}$$\n",
        "\n",
        "And finally, we can write our loss function - the cross-entropy loss - exactly as in logistic regression:\n",
        "\n",
        "$$ \\mathcal{L}(W,U,\\mathbf{b}) = \\sum_{(x,\\ y\\ =\\ j)\\ \\in\\ \\mathcal{D}} -\\log \\hat{y}_j(x) $$\n",
        "\n",
        "Because we train both the input features $W$ and the class features $U$ at the same time, this problem is no longer convex. However, as with neural networks, we can still train it using gradient descent. In the cells below, we'll implement the above equations in TensorFlow, and use the powerful automatic differentiation features to set up training with just a few lines of code."
      ]
    },
    {
      "metadata": {
        "id": "1ipbfUH3rHpK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Constructing the model\n",
        "\n",
        "A TensorFlow implemenation of CBOW follows. We'll use the bAbI corpus as before, but encourage you to experiment with other text corpora."
      ]
    },
    {
      "metadata": {
        "id": "PQ799W7wrPaK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To implement CBOW in TensorFlow, we need to define a Tensor for each model component. To clearly distinguish between TF and non-TF code, TF object names will be suffixed by an underscore (\"_\"). We also will construct the model such that it accept batch inputs to speed up training.\n",
        "\n",
        "**Hyperparameters**\n",
        "* V : vocabulary size\n",
        "* M : embedding size\n",
        "* N : context size (excluding the center word)\n",
        "\n",
        "**Inputs**\n",
        "* ids_ : (batch_size, N), integer indices for context words\n",
        "* y_ : (batch_size,), integer indices for target word\n",
        "\n",
        "**Model Parameters**\n",
        "* V_ : (V,M), input-side word embeddings\n",
        "* U_ : (V,M), output-side word embeddings\n",
        "* b_ : (V,), biases for output-side\n",
        "\n",
        "**Intermediate States**\n",
        "* x_ : (batch_size, N, M), concatenated word embeddings of the context words\n",
        "* z_ : (batch_size, M), average of context words' embeddings\n",
        "* logits_ : (batch_size, V), $zU^T + b$"
      ]
    },
    {
      "metadata": {
        "id": "1fBsKZCHrSX6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(42)\n",
        "\n",
        "# MODEL HYPERPARAMETERS\n",
        "V = vocab.size\n",
        "M = 25\n",
        "N = 4\n",
        "\n",
        "# INPUTS\n",
        "# Using \"None\" in place of batch size allows it to be dynamically computed later.\n",
        "with tf.name_scope(\"Inputs\"):\n",
        "    ids_ = tf.placeholder(tf.int32, shape=[None, N], name=\"ids\")\n",
        "    y_ = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
        "\n",
        "# EMBEDDING LAYER COMPONENTS\n",
        "# Here we grab the embedddings of the context words and sum them.\n",
        "# All of the input embeddings are stored in V_.\n",
        "with tf.name_scope(\"Embedding_Layer\"):\n",
        "    # Use random initialization for the input embeddings.\n",
        "    V_ = tf.Variable(tf.random_uniform([V, M]), name=\"V\")\n",
        "\n",
        "    # We want the embedding_lookup to produce shape: (batch_size, N, M).\n",
        "    # Passing [-1, N, M] to tf.reshape will fill in the batch_size dynamically.\n",
        "    x_ = tf.reshape(tf.nn.embedding_lookup(V_, ids_), \n",
        "                    [-1, N, M], name=\"x\")\n",
        "    \n",
        "    # We want to sum over the words in the window (N) for each example,\n",
        "    # resulting in a shape for z_: (batch_size, M).\n",
        "    z_ = tf.reduce_sum(x_, axis=1)\n",
        "\n",
        "# OUTPUT LAYER COMPONENTS\n",
        "with tf.name_scope(\"Output_Layer\"):\n",
        "    U_ = tf.Variable(tf.random_uniform([V,M]), name=\"U\")\n",
        "    b_ = tf.Variable(tf.zeros([V,], dtype=tf.float32), name=\"b\")\n",
        "    logits_ = tf.add(tf.matmul(z_, tf.transpose(U_)), b_, name=\"logits\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "34R-x-ttrdUB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sampled Softmax\n",
        "\n",
        "We'll add in our usual cross-entropy loss. The full **softmax** is computationally extremely slow because the denominator is a sum over the probabilities of every word in the vocabulary. To speed up training we'll use a clever trick called the **sampled softmax** loss, as in Jozefowicz et al. 2016. Sampled softmax approximates the denominator by replacing the exhaustive sum with a sum of a sample of random center words.\n",
        "\n",
        "(Note: running the following code cell may give a warning that the softmax_cross_entropy_with_logits function is being deprecated and will be removed in a future version; you can ignore this warning for now.)"
      ]
    },
    {
      "metadata": {
        "id": "8dX6rwjcrfw6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"Cost_Function\"):\n",
        "    # Sampled softmax loss, for training\n",
        "    per_example_train_loss_ = tf.nn.sampled_softmax_loss(weights=U_, biases=b_,\n",
        "                                             labels=tf.expand_dims(y_, 1), inputs=z_,\n",
        "                                             num_sampled=100, num_classes=V,\n",
        "                                             name=\"per_example_sampled_softmax_loss\")\n",
        "    train_loss_ = tf.reduce_mean(per_example_train_loss_, name=\"sampled_softmax_loss\")\n",
        "    \n",
        "    # Full softmax loss, for scoring\n",
        "    per_example_loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=y_, logits=logits_, name=\"per_example_loss\")\n",
        "    loss_ = tf.reduce_mean(per_example_loss_, name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cw9Mvxhsrn3-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's add in our training ops. We will use AdaGrad instead of SGD; this is faster because it keeps track of how many times we have seen a particular example and modifying the learning rate accordingly. Why is this useful? Let's take the words \"the\" and \"aardvark.\" We are likely going to see the word \"the\" many many more times than \"aardvark\". After seeing the word \"the\" a thousand times, we probably don't need to backpropagate the errors as much anymore because we've probably converged to the correct embedding. However, we don't see the word \"aardvark\" very often, so we'd like to keep the learning rate fairly high so that we can converge to its embedding faster. AdaGrad allows us to do this instead of keeping a single learning rate for all examples."
      ]
    },
    {
      "metadata": {
        "id": "sp9AqlHerp1x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"Training\"):\n",
        "    alpha_ = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
        "    optimizer_ = tf.train.AdagradOptimizer(alpha_)\n",
        "    train_step_ = optimizer_.minimize(train_loss_)\n",
        "    \n",
        "# Initializer step\n",
        "init_ = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kXVj8R0arurk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's add a few more ops to do our final predictions after we have completed training:\n",
        "\n",
        "* pred\\_proba_ : (batch_size, V), $ = P(w_i | w_{i-1}, w_{i+1} ...)$ for all words $i$\n",
        "* pred_max : (batch_size,), id of most likely middle word\n",
        "* pred_random : (batch_size,), id of a randomly-sampled middle word"
      ]
    },
    {
      "metadata": {
        "id": "cuUaJDTOrvbd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"Prediction\"):\n",
        "    pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
        "    pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
        "    pred_random_ = tf.multinomial(logits_, 1, name=\"pred_random\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4LomRh3TsADL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Batches\n",
        "\n",
        "We will also need a function to generate batches, which will be used in the next section. Each batch consists of `batch_size` training examples, where each includes an array of context word ids, and a single id representing the middle (target) word."
      ]
    },
    {
      "metadata": {
        "id": "mHIYmh5RsE1z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create windows of words -- we'll use the middle word as\n",
        "# the target, and the others as the input context.\n",
        "train_windows = build_windows(train_ids, N)\n",
        "test_windows = build_windows(test_ids, N)\n",
        "print 'Training windows shape:', train_windows.shape\n",
        "print 'Test windows shape:', test_windows.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8a7UqmZtsIR8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cbow_slice_batch(batch, N):\n",
        "    \"\"\"Extract center words as targets, left and right context as inputs.\"\"\"\n",
        "    # Get the center word from each example as the targets.\n",
        "    targets = batch[:,N//2]  # The // operator does integer division\n",
        "    \n",
        "    # Get the context words as inputs.\n",
        "    inputs = np.hstack([batch[:,:N//2], batch[:,(N//2+1):]])\n",
        "    return inputs, targets\n",
        "\n",
        "def train_batch(session, batch, alpha):\n",
        "    inputs, targets = cbow_slice_batch(batch, N)\n",
        "    # Prepare inputs for TensorFlow.\n",
        "    feed_dict = {ids_:inputs, y_:targets, alpha_:alpha}\n",
        "    c, _ = session.run([train_loss_, train_step_],\n",
        "                       feed_dict=feed_dict)\n",
        "    return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ge_jquudsPLV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Nearest neighbors\n",
        "\n",
        "We're almost ready to train the model. With the hyper-parameters we've chosen, training should go fairly quickly, converging to reasonably good results after around 10 epochs.\n",
        "\n",
        "Besides reporting the loss, we can make some sense of our progress by examining nearest neighbors of a few test words in the embedding space. We'll compute cosine similarity between each of these words and the rest of the vocabulary, and show a few nearest neighbors. The semantic similarity between nearest neighbors should improve as the model trains."
      ]
    },
    {
      "metadata": {
        "id": "EmEAYcGNsSBy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_nearest_neighbors = 5 # Number of closest neighbors to print\n",
        "test_words = [\"fred\", \"garden\", \"mice\", \"where\", \"afterwards\"]\n",
        "\n",
        "def print_nearest_neighbors(test_words):\n",
        "    word_ids = np.array(vocab.words_to_ids(test_words))\n",
        "    words_ = tf.constant(word_ids, dtype=tf.int32)\n",
        "\n",
        "    # Normalize all embeddings to unit length for better comparisons.\n",
        "    normalized_embeddings_ = V_ / tf.sqrt(tf.reduce_sum(tf.square(V_), 1, keepdims=True))\n",
        "\n",
        "    # Get the normalized embeddings for the test words.\n",
        "    words_normalized_embeddings_ = tf.nn.embedding_lookup(normalized_embeddings_, words_)\n",
        "    \n",
        "    # Cosine similarity is just a dot-product once embeddings have been normalized.\n",
        "    # Remember that similarity between identical vectors is 1.\n",
        "    similarity_ = tf.matmul(words_normalized_embeddings_, tf.transpose(normalized_embeddings_))\n",
        "    \n",
        "    # Execute the tf operations with a session.\n",
        "    evaluated_sim_ = similarity_.eval(session=session)\n",
        "\n",
        "    # Print the words with the largest similarities.\n",
        "    print \"Nearest neighbors of test words:\"\n",
        "    for i in xrange(len(test_words)):\n",
        "        # The argsort() function returns the indices of a vector corresponding\n",
        "        # to sorted order (from smallest to largest).\n",
        "        nearest = (-evaluated_sim_[i, :]).argsort()[1:num_nearest_neighbors+1]\n",
        "        print \"  '{:s}' -> \".format(test_words[i]),\n",
        "        for k in xrange(num_nearest_neighbors):\n",
        "            closest_word = vocab.id_to_word[nearest[k]]\n",
        "            print \"'{:s}'\".format(closest_word),\n",
        "        print \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qAqgf1OPsbX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TRAINING PARAMETERS\n",
        "num_epochs = 10 # one epoch = one pass through the training data\n",
        "batch_size = 256\n",
        "alpha = 1.0 # learning rate\n",
        "\n",
        "print_every = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0P0gKfRnscEc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(42) # we set this for reproducability\n",
        "session = tf.Session()\n",
        "session.run(init_)\n",
        "\n",
        "t0 = time.time()\n",
        "for epoch in xrange(1, num_epochs+1):\n",
        "    t0_epoch = time.time()\n",
        "    epoch_cost = 0.0\n",
        "    total_batches = 0\n",
        "    print \"\"\n",
        "    for i, batch in enumerate(batch_generator(train_windows, batch_size)):\n",
        "        if (i % print_every == 0):\n",
        "            print \"[epoch {:d}] seen {:,} minibatches\".format(epoch, i)\n",
        "        epoch_cost += train_batch(session, batch, alpha)\n",
        "        total_batches = i + 1\n",
        "\n",
        "    avg_cost = epoch_cost / total_batches\n",
        "    print \"[epoch {:d}] Completed {:d} minibatches in {:s}\".format(epoch, i, pretty_timedelta(since=t0_epoch))\n",
        "    print \"[epoch {:d}] Average cost: {:.03f}\".format(epoch, avg_cost)\n",
        "    print_nearest_neighbors(test_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w4N4I0z9sjIE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample_test_words = [\"bathroom\", \"lion\", \"cinema\", \"jessica\"]\n",
        "print_nearest_neighbors(sample_test_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jPvXoZkgskO8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Scoring\n",
        "\n",
        "We'll score our model the same as the n-gram model, by computing perplexity over the dev set.\n",
        "\n",
        "Recall that perplexity is just the exponentiated average cross-entropy loss:\n",
        "\n",
        "$$ \\text{Perplexity} = \\left( \\prod_i \\frac{1}{Q(x_i)} \\right)^{1/N} = \\left( \\prod_i 2^{- \\log_2 Q(x_i)} \\right)^{1/N} = 2^{\\left(\\frac{1}{N} \\sum_i -\\log_2 Q(x_i)\\right)} = 2^{\\tilde{CE}(P,Q)}$$\n",
        "\n",
        "In practice, TF uses the natural log, so the loss will be scaled by a factor of $\\ln 2$, but the base cancels and the perplexity scores will be the same.\n",
        "\n",
        "Note that below we use loss_, which is the cross-entropy loss with the full softmax."
      ]
    },
    {
      "metadata": {
        "id": "EEU6YcN6snTG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def score_batch(session, batch):\n",
        "    inputs, targets = cbow_slice_batch(batch, N)\n",
        "    feed_dict = {ids_:inputs, y_:targets}\n",
        "    return session.run(loss_, feed_dict=feed_dict)\n",
        "\n",
        "def score_dataset(data):\n",
        "    total_cost = 0.0\n",
        "    total_batches = 0\n",
        "    for batch in batch_generator(data, 1000):\n",
        "        total_cost += score_batch(session, batch)\n",
        "        total_batches += 1\n",
        "\n",
        "    return total_cost / total_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GAU9QyNQspxQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print \"Train set perplexity: %.03f\" % np.exp(score_dataset(train_windows))\n",
        "print \"Test set perplexity: %.03f\" % np.exp(score_dataset(test_windows))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jg16a6sXstJo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Predicting center words\n",
        "\n",
        "Let's use our trained model to try predicting the middle word in some example sentences. We'll provide a context (two words to the left and right) and ask our model to predict the word in the middle. Note that we can use the `pred_max_` function to retrieve the word with the largest probability, or the `pred_random_` function to sample from the predicted distribution over words. The former is deterministic, while the latter will give a different result with each run.\n",
        "\n",
        "While our model produces reasonable embeddings, it's not very good at predicting the target word."
      ]
    },
    {
      "metadata": {
        "id": "oz8BW6ozsyIo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentences = [\"mary went to the cinema\",\n",
        "             \"brian is carrying a football\"]\n",
        "\n",
        "# This function feeds our sequence of words into the model and \n",
        "# runs it to get a guess of the center word. Feed the entire\n",
        "# sentence in as it will automatically strip away the middle word\n",
        "# before it enters the context words into the model.\n",
        "def predict_target(session, seq):\n",
        "    inputs, _ = cbow_slice_batch(np.array(seq).reshape([1, -1]), N)\n",
        "    middle_word_id = session.run(pred_max_, feed_dict={ids_:inputs}).flatten()\n",
        "    return middle_word_id[0]\n",
        "\n",
        "for sentence in sentences:\n",
        "    words = sentence.split()\n",
        "    for w in words:\n",
        "        if not w in vocab.wordset: raise KeyError(\"Out-of-vocabulary word \\\"{:s}\\\"\".format(w))\n",
        "    \n",
        "    sentence_ids = vocab.words_to_ids(words)\n",
        "    pred_id = predict_target(session, sentence_ids)\n",
        "    print \"Actual sentence:  \\\"{:s}\\\"\".format(sentence)\n",
        "    print \"Sampled sentence: \\\"{:s} [{:s}] {:s}\\\"\".format(\" \".join(words[:N//2]),\n",
        "                                                          vocab.id_to_word[pred_id],\n",
        "                                                          \" \".join(words[N//2+1:]))\n",
        "    print"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QRA_sR6xs7Mt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Takeaways\n",
        "\n",
        "We've demonstrated the CBOW model for producing word embeddings by predicting center words given context words. Whereas applying SVD captures maximum variance in the co-occurrence matrix, CBOW chooses embeddings that are suited to prediction. This means that CBOW can learn what aspects or dimensions are useful for a task, which turns out to be important. So why should we care about producing these embeddings? There are two big reasons.\n",
        "\n",
        "* **Embeddings are the foundation for most Neural Network models of any language task.** Usually, the end goal is not the embeddings themselves. Instead, we are more likely to care about something like classification of documents. In this case, we'd create a network where the first layer is an embedding lookup and the final layer is a softmax over the possible classes. In between, there are a variety of possible network achitectures, but they all typically start with some kind of embeddings.\n",
        "\n",
        "* **Pre-trained embeddings facilitate transfer learning.** Transfer learning is the transfer of knowledge about one task to help with another task. For example, consider a classification task with only a few thousand training examples. That's not enough to learn the parameters for word embeddings (for any reasonable sized vocabulary). Instead, we can initialize our model with embeddings trained from a large corpus of text. This strategy often gives dramatic improvements in results. It's a form of semi-supervised learning. We can estimate word embeddings from heaps of unlabeled text, and apply that learned knowledge -- representations of meaning for words -- to new tasks with a little labeled data."
      ]
    },
    {
      "metadata": {
        "id": "7h-PUh-QtBdw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experiments\n",
        "\n",
        "(1) In our implementation, we estimated separate (untied) parameters for the $V$ and $U$ matrices, the 'input' and 'output' embeddings. Modify the code to only use a single set of parameters. How does this affect the performance (test perplexity)?\n",
        "\n",
        "(2) Modify the CBOW code here to implement SkipGram. Rather than averaging the context words to predict the center word, SkipGram takes a center word as input and tries to predict the context words."
      ]
    }
  ]
}