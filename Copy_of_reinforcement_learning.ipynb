{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of reinforcement_learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "qv0Xp2ZHDOq7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrywoo/MyML/blob/master/Copy_of_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qv0Xp2ZHDOq7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Copyright 2018 Google LLC."
      ]
    },
    {
      "metadata": {
        "id": "Ja8W3KL-DQ4j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qc-lJM2jDTRr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "In this lab, we define an agent and environment using [OpenAI Gym](https://gym.openai.com/), a toolkit for developing reinforcement learning algorithms, which is compatible with TensorFlow. Gym provides us with a collection of [environments](https://gym.openai.com/docs/#environments), such as:\n",
        "\n",
        "*   [Classic control](https://gym.openai.com/envs/#classic_control): control theory problems from the classic RL literature\n",
        "*   [Toy text](https://gym.openai.com/envs/#toy_text): simple text environments\n",
        "*   [Algorithms](https://gym.openai.com/envs/#algorithmic): learn to imitate computations\n",
        "*   [Atari](https://gym.openai.com/envs/#atari): Atari 2600 games\n",
        "*   [Box2D](https://gym.openai.com/envs/#box2d): continuous control tasks in the Box2D simulator\n",
        "*   [Robotics](https://gym.openai.com/envs/#robotics): simulated goal-based tasks for the Fetch and ShadowHand robots\n",
        "*   [MuJoCo](https://gym.openai.com/envs/#mujoco): continuous control tasks, running in a fast physics simulator\n",
        "\n",
        "We start by installing some libraries, then define the agent and environment, train an agent on CartPole, and one on Atari.\n",
        "\n",
        "## Outline\n",
        "  1. Setup\n",
        "  1. Episodes\n",
        "  1. Summaries\n",
        "  1. Observations\n",
        "  1. Agent\n",
        "  1. Experiment\n",
        "  1. Task 1: Training an agent on CartPole\n",
        "  1. Task 2: Training an agent on Atari\n",
        "\n",
        "\n",
        "Please **make a copy** of this Colab notebook before starting this lab. To do so, choose **File**->**Save a copy in Drive**."
      ]
    },
    {
      "metadata": {
        "id": "w6ZWy5D7DfGE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Let's get started by installing some libraries."
      ]
    },
    {
      "metadata": {
        "id": "fSKX8cdrBiTs",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for open ai gym\n",
        "!pip install -q gym\n",
        "!apt -qq install cmake\n",
        "!pip install \"gym[atari, classic_control]\"\n",
        "# !apt-get install python-opengl\n",
        "\n",
        "# for viualization\n",
        "!pip install git+https://github.com/jakevdp/JSAnimation.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pe8k9LxdOsEt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We then load a helper function for visualizations."
      ]
    },
    {
      "metadata": {
        "id": "9Znp38vU-_8k",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Source: https://github.com/jakevdp/JSAnimation\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    display(display_animation(anim, default_mode='loop'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Czsll0jyOxAm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, run the following code cell to import the necessary libraries."
      ]
    },
    {
      "metadata": {
        "id": "IbEy-u2MGxhy",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Se3PrgWSO25X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're now ready to start defining the following:\n",
        "\n",
        "*   Episodes\n",
        "*   Summaries\n",
        "*   Observations\n",
        "*   The agent\n",
        "*   The experiment"
      ]
    },
    {
      "metadata": {
        "id": "MXdmtiNy2Ddj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Episodes\n",
        "\n",
        "We start by defining episodes, which are made up of:\n",
        "\n",
        "*   actions\n",
        "*   observations\n",
        "*   rewards\n",
        "*   length"
      ]
    },
    {
      "metadata": {
        "id": "NNPiQQn-Gu2N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Episode = collections.namedtuple(\"Episode\", [\"actions\", \"observations\", \"rewards\", \"length\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xRZ-jZZMS2et",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following function which creates a collection of episodes will be used when we define an experiment."
      ]
    },
    {
      "metadata": {
        "id": "Xn3q3989j0BR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def collect_n_episodes(num_ep, max_ep_len, env, agent, session, animate=False):\n",
        "    episodes = []\n",
        "    frames = []\n",
        "    for ep in range(num_ep):\n",
        "        obs = env.reset()\n",
        "        observations, actions, rewards = [], [], []\n",
        "\n",
        "        for step in range(int(max_ep_len)):\n",
        "            observations.append(obs)\n",
        "            action = session.run(agent.sampled_action, feed_dict={agent.obs_ph: obs[None]})\n",
        "            # squeeze the batch dimension, hence action[0]\n",
        "            action = action[0]\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            if animate:\n",
        "              frames.append(env.render(mode='rgb_array'))          \n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "        episodes.append(Episode(actions=actions, observations=observations, rewards=rewards, length=len(actions)))\n",
        "    \n",
        "    if animate:\n",
        "      return frames\n",
        "    \n",
        "    return episodes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z0ktrQ6SkECC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Summaries\n",
        "\n",
        "We then define and compute summaries,  which are made up of:\n",
        "\n",
        "*   time elapsed\n",
        "*   number of global steps\n",
        "*   policy loss\n",
        "*   mean\n",
        "*   standard deviation\n",
        "*   min and max return\n",
        "*   mean and standard deviation for episode length"
      ]
    },
    {
      "metadata": {
        "id": "XkGfxf2v3gip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Summary = collections.namedtuple(\"Summary\",\n",
        "                                 [\"time_elapsed\", \"global_steps\", \"mean_return\", \"std_return\", \"min_return\",\n",
        "                                  \"max_return\", \"mean_episode_len\",\n",
        "                                  \"std_episode_len\", \"policy_loss\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "92w42337S_ql",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following function will be called to create a summary when we run an experiment."
      ]
    },
    {
      "metadata": {
        "id": "tFHLLSMnkFay",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_summary(episodes, time_elapsed, policy_loss, global_steps):\n",
        "    returns = [np.sum(e.rewards) for e in episodes]\n",
        "    ep_lengths = [e.length for e in episodes]\n",
        "    return Summary(time_elapsed=time_elapsed, global_steps=global_steps, policy_loss=policy_loss,\n",
        "                   mean_return=np.mean(returns),\n",
        "                   std_return=np.std(returns), max_return=np.max(returns), min_return=np.min(returns),\n",
        "                   mean_episode_len=np.mean(ep_lengths), std_episode_len=np.std(ep_lengths))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KjJPwXaTkHR8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Observations\n",
        "\n",
        "We then define a function to encode observations."
      ]
    },
    {
      "metadata": {
        "id": "LrPE_mimkJDm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encode_observation(obs_ph, output_size, scope, n_layers=2, size=32, activation=tf.tanh, output_activation=None):\n",
        "    with tf.variable_scope(scope):\n",
        "        dense = obs_ph\n",
        "        for i in range(n_layers):\n",
        "            dense = tf.layers.dense(inputs=dense, units=size, activation=activation)\n",
        "        return tf.layers.dense(inputs=dense, units=output_size, activation=output_activation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pz_3keWtkLmS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The agent\n",
        "\n",
        "We now define the Actor-Critic agent."
      ]
    },
    {
      "metadata": {
        "id": "rtUd4kMW3lRX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "A2cAgent = collections.namedtuple(\"A2cAgent\",\n",
        "                                  [\"obs_ph\", \"action_ph\", \"advantage_ph\", \"value_ph\", \"sampled_action\",\n",
        "                                   \"value_prediction\", \"critic_train_op\", \"policy_loss\", \"policy_train_op\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-FteEoPf2A95",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_network(env, learning_rate=5e-3, n_layers=1, size=3, entropy_weight=0.0):\n",
        "    discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
        "\n",
        "    obs_ph = tf.placeholder(shape=[None, obs_dim], name=\"obs_ph\", dtype=tf.float32)\n",
        "    if discrete:\n",
        "        action_ph = tf.placeholder(shape=[None], name=\"action_ph\", dtype=tf.int32)\n",
        "    else:\n",
        "        action_ph = tf.placeholder(shape=[None, action_dim], name=\"action_ph\", dtype=tf.float32)\n",
        "\n",
        "    advantage_ph = tf.placeholder(shape=[None], name=\"advantage_ph\", dtype=tf.float32)\n",
        "\n",
        "    if discrete:\n",
        "        action_logits = encode_observation(\n",
        "            obs_ph=obs_ph,\n",
        "            output_size=action_dim,\n",
        "            scope=\"action_logits\",\n",
        "            n_layers=n_layers,\n",
        "            size=size,\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        sampled_action = tf.squeeze(tf.multinomial(action_logits, 1), axis=[1])\n",
        "        entropy = tf.reduce_sum(-tf.nn.softmax(action_logits) * tf.nn.log_softmax(action_logits), axis=-1)\n",
        "        neg_log_probs = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=action_ph, logits=action_logits)\n",
        "\n",
        "    else:\n",
        "        action_mean = encode_observation(\n",
        "            obs_ph=obs_ph,\n",
        "            output_size=action_dim,\n",
        "            scope=\"action_mean\",\n",
        "            n_layers=n_layers,\n",
        "            size=size,\n",
        "            activation=tf.nn.relu)\n",
        "\n",
        "        action_log_stdev = tf.get_variable(\"log_stdev\", shape=[action_dim])\n",
        "        sampled_action = action_mean + tf.multiply(tf.exp(action_log_stdev),\n",
        "                                                   tf.random_normal(tf.shape(action_mean)))\n",
        "        distribution = tf.contrib.distributions.MultivariateNormalDiag(loc=action_mean,\n",
        "                                                                       scale_diag=tf.exp(action_log_stdev))\n",
        "        neg_log_probs = -distribution.log_prob(action_ph)\n",
        "        entropy = distribution.entropy()\n",
        "\n",
        "    weighted_neg_log_probs = tf.multiply(neg_log_probs, advantage_ph)\n",
        "    policy_loss = tf.reduce_mean(weighted_neg_log_probs - entropy * entropy_weight)\n",
        "    policy_train_op = tf.train.AdamOptimizer(learning_rate).minimize(policy_loss)\n",
        "\n",
        "    # compute critic loss\n",
        "    value_prediction = tf.squeeze(encode_observation(\n",
        "        obs_ph=obs_ph,\n",
        "        output_size=1,\n",
        "        scope=\"value_prediction\",\n",
        "        n_layers=n_layers,\n",
        "        size=size))\n",
        "\n",
        "    value_ph = tf.placeholder(shape=[None], dtype=tf.float32)\n",
        "    critic_loss = tf.losses.mean_squared_error(predictions=value_prediction, labels=value_ph)\n",
        "    critic_train_op = tf.train.AdamOptimizer(learning_rate).minimize(critic_loss)\n",
        "\n",
        "    return A2cAgent(obs_ph=obs_ph, action_ph=action_ph, advantage_ph=advantage_ph, value_ph=value_ph,\n",
        "                    sampled_action=sampled_action,\n",
        "                    critic_train_op=critic_train_op, value_prediction=value_prediction,\n",
        "                    policy_loss=policy_loss, policy_train_op=policy_train_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MBOBD__UkVBD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experiment\n",
        "\n",
        "Finally, we define an experiment for an agent and environment. This function takes as input an [environment](https://gym.openai.com/envs/#classic_control) and a set of parameters for the neural network. It returns a trained agent, which can be used for evaluation, and episode summaries.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "u6ZaDrwwkWxy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_experiment(env_name='CartPole-v0',\n",
        "                   n_iter=100,\n",
        "                   num_ep_per_iter=34,\n",
        "                   gamma=1.0,\n",
        "                   max_episode_len=None,\n",
        "                   learning_rate=5e-3,\n",
        "                   entropy_weight=0.0,\n",
        "                   enable_critic=False,\n",
        "                   seed=0,\n",
        "                   n_layers=1,\n",
        "                   size=32\n",
        "                   ):\n",
        "    start = time.time()\n",
        "\n",
        "    # Set random seeds\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Make gym environment\n",
        "    env = gym.make(env_name)\n",
        "    max_episode_len = max_episode_len or env.spec.max_episode_steps\n",
        "\n",
        "    # build neural nets\n",
        "    agent = build_network(env=env, learning_rate=learning_rate, n_layers=n_layers, size=size,\n",
        "                          entropy_weight=entropy_weight)\n",
        "\n",
        "    sess = tf.Session()\n",
        "    sess.__enter__()\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    total_timesteps = 0\n",
        "    summaries = []\n",
        "    for itr in range(n_iter):\n",
        "\n",
        "        episodes = collect_n_episodes(num_ep=num_ep_per_iter, max_ep_len=max_episode_len, env=env, agent=agent,\n",
        "                                      session=sess)\n",
        "\n",
        "        # [batch_size, obs_dim]\n",
        "        obs_input = np.concatenate([e.observations for e in episodes])\n",
        "        # [batch_size, action_dim]\n",
        "        action_input = np.concatenate([e.actions for e in episodes])\n",
        "\n",
        "        def discount_rewards_to_go(rewards, gamma):\n",
        "            res = []\n",
        "            future_reward = 0\n",
        "            for r in reversed(rewards):\n",
        "                future_reward = future_reward * gamma + r\n",
        "                res.append(future_reward)\n",
        "            return res[::-1]\n",
        "\n",
        "        # [batch_size]\n",
        "        state_action_values = np.concatenate([discount_rewards_to_go(e.rewards, gamma) for e in episodes])\n",
        "\n",
        "        if enable_critic:\n",
        "            state_values = sess.run(agent.value_prediction, feed_dict={agent.obs_ph: obs_input})\n",
        "            advantages = state_action_values - state_values\n",
        "        else:\n",
        "            advantages = state_action_values.copy()\n",
        "\n",
        "        advantages = (advantages - np.mean(advantages)) / np.std(advantages)\n",
        "\n",
        "        if enable_critic:\n",
        "            sess.run(agent.critic_train_op,\n",
        "                     feed_dict={agent.obs_ph: obs_input, agent.value_ph: state_action_values})\n",
        "\n",
        "        _, policy_loss_value = sess.run([agent.policy_train_op, agent.policy_loss],\n",
        "                                        feed_dict={agent.obs_ph: obs_input, agent.action_ph: action_input,\n",
        "                                                   agent.advantage_ph: advantages})\n",
        "\n",
        "        # Log diagnostics\n",
        "        batch_size = np.sum([e.length for e in episodes])\n",
        "        total_timesteps += batch_size\n",
        "        time_elapsed = time.time() - start\n",
        "        summary = compute_summary(episodes, time_elapsed, policy_loss_value, total_timesteps)\n",
        "        \n",
        "        print(summary)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    # return the trained agent, so we can use it for evaluation.\n",
        "    # also return summary for plotting\n",
        "    return agent, summaries, sess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D78hpbAG3Tof",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1: Train an agent on CartPole\n",
        "\n",
        "Run the following code cell to tune the agent on [CartPole](https://gym.openai.com/envs/CartPole-v1/), modifying the different parameters' values."
      ]
    },
    {
      "metadata": {
        "id": "pGL94BclgrH4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env_name = 'CartPole-v0' #@param [\"CartPole-v0\", \"MountainCarContinuous-v0\"]\n",
        "num_iterations =100 #@param {type:\"slider\", min:1, max:300, step:1}\n",
        "num_episodes_per_iteration = 6 #@param {type:\"slider\", min:1, max:300, step:1}\n",
        "discount_factor =0.99 #@param {type:\"slider\", min:0.01, max:1, step:0.01}\n",
        "max_episode_length = 154 #@param {type:\"slider\", min:0, max:500, step:1}\n",
        "entropy_weight = 0.011 #@param {type:\"slider\", min:0.0, max:0.1, step:0.001}\n",
        "learning_rate =5e-3 #@param\n",
        "random_seed = 23 #@param\n",
        "number_of_layers = 3 #@param {type:\"slider\", min:0, max:20, step:1}\n",
        "number_of_neuron_per_layer = 32 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "enable_critic = True #@param {type:\"boolean\"}\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "  run_experiment(env_name=env_name,\n",
        "               n_iter=num_iterations,\n",
        "               num_ep_per_iter=num_episodes_per_iteration,\n",
        "               gamma=discount_factor,\n",
        "               max_episode_len=max_episode_length,\n",
        "               learning_rate=learning_rate,\n",
        "               enable_critic=enable_critic,\n",
        "               entropy_weight = entropy_weight,\n",
        "               seed=random_seed,\n",
        "               n_layers=number_of_layers,\n",
        "               size=number_of_neuron_per_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-188J6eOpHB0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 2: Train the agent on Atari\n",
        "\n",
        "Now train an agent on the Atari 2600 game [Pong](https://gym.openai.com/envs/Pong-ram-v0/).\n",
        "\n",
        "The following code will help with the visualization."
      ]
    },
    {
      "metadata": {
        "id": "gBzrEs0JkXQr",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env_name = 'Pong-ram-v0'\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "  agent, _, session = run_experiment(env_name=env_name,\n",
        "               n_iter=1,\n",
        "               num_ep_per_iter=32,\n",
        "               gamma=0.99,\n",
        "               max_episode_len=150,\n",
        "               learning_rate=5e-3,\n",
        "               enable_critic=False,\n",
        "               entropy_weight = 0.012,\n",
        "               seed=3,\n",
        "               n_layers=10,\n",
        "               size=30)\n",
        "\n",
        "  frames = collect_n_episodes(1, 200, gym.make(env_name), \n",
        "                              agent, session, animate=True)\n",
        "  \n",
        "  display_frames_as_gif(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}